{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3214db25",
   "metadata": {},
   "source": [
    "# âœ¨ Bases de Datos EMG - Grupo 02\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Tema del Proyecto\n",
    "**Hand Gesture Recognition with EMG Signals**\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ ProblemÃ¡tica  \n",
    "Los pacientes que han sufrido un **ictus** enfrentan serias dificultades para recuperar el control motor fino de sus manos.  \n",
    "Una herramienta que permita **monitorear la recuperaciÃ³n motora** mediante el reconocimiento de gestos con seÃ±ales EMG serÃ­a de gran utilidad.  \n",
    "\n",
    "Actualmente, los sistemas disponibles son:  \n",
    "- ğŸ’° **Costosos**  \n",
    "- âš™ï¸ **Complejos de implementar**  \n",
    "- ğŸš« **Poco accesibles en contextos clÃ­nicos comunes**  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Propuesta de SoluciÃ³n  \n",
    "âœ… Recopilar seÃ±ales **EMG** desde los mÃºsculos del antebrazo.  \n",
    "âœ… Entrenar un **clasificador supervisado** (*KNN, SVM, etc.*).  \n",
    "âœ… Reconocer entre **3 y 5 gestos sencillos de la mano** para su aplicaciÃ³n en rehabilitaciÃ³n.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Objetivos de esta SecciÃ³n (Bases de Datos)\n",
    "\n",
    "ğŸ“‚ **Explorar** repositorios pÃºblicos con seÃ±ales EMG.  \n",
    "ğŸ“Š **Comparar** sus caracterÃ­sticas principales (frecuencia de muestreo, nÃºmero de sujetos, cantidad de gestos).  \n",
    "ğŸ” **Seleccionar** el dataset mÃ¡s adecuado para entrenar y validar el clasificador.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03625bad",
   "metadata": {},
   "source": [
    "# ğŸ“‘ Base de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7bbc7",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£1ï¸âƒ£ [Gesture Recognition and Biometrics ElectroMyogram (GRABMyo)](https://physionet.org/content/grabmyo/1.1.0/ ) ğŸ“š\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ InformaciÃ³n General\n",
    "- **Nombre completo:** *Gesture Recognition and Biometrics ElectroMyogram (GRABMyo)*  \n",
    "- **Autores:** Ning Jiang, Ashirbad Pradhan, Jiayuan He  \n",
    "- **Publicado en:** PhysioNet (versiÃ³n 1.1.0, junio 7, 2024)  \n",
    "- **DOI:** [10.13026/89dm-f662](https://doi.org/10.13026/89dm-f662)  \n",
    "- **Sujetos:** 43 participantes (23 hombres, 20 mujeres), edad: 24â€“35 aÃ±os  \n",
    "- **Sesiones:** 3 dÃ­as distintos â†’ *129 grabaciones en total*  \n",
    "- **Gestos registrados:** 16 gestos de mano y dedos + reposo  \n",
    "- **Frecuencia de muestreo:** 2048 Hz  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://physionet.org/files/grabmyo/1.1.0/GestureList.JPG?download\" alt=\"gramb_refernce\" height=\"700\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ PropÃ³sito del Dataset\n",
    "El dataset se diseÃ±Ã³ para aplicaciones en:  \n",
    "1. ğŸ” **BiometrÃ­a EMG**: identificaciÃ³n y autenticaciÃ³n personal.  \n",
    "2. âœ‹ **Reconocimiento de gestos**: rehabilitaciÃ³n, prÃ³tesis, control de interfaces y entornos virtuales.  \n",
    "3. ğŸ§ª **Robustez multisesiÃ³n**: evaluar variaciones entre dÃ­as (electrodos, piel, fatiga, etc.).  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© DiseÃ±o Experimental\n",
    "- **Electrodos:** 28 en total (16 antebrazo + 12 muÃ±eca en configuraciÃ³n bipolar).  \n",
    "- **Protocolo:**  \n",
    "  - 17 gestos (16 movimientos + reposo).  \n",
    "  - Cada gesto repetido **7 veces**.  \n",
    "  - DuraciÃ³n: 5 s por gesto + 10 s de descanso.  \n",
    "- **ConfiguraciÃ³n de sesiones:**  \n",
    "  - `Session1/Session2/Session3`  \n",
    "  - Cada una con 43 subcarpetas (un sujeto por carpeta).  \n",
    "  - Archivos de salida: `.dat` y `.hea`.  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ieee-dataport.org/sites/default/files/styles/home/public/Appendix1_2.jpg?itok=Z4M-sNmQ\" alt=\"gramb_refernce2\" height=\"300\">\n",
    "</p>\n",
    "---\n",
    "\n",
    "## ğŸ“Š CaracterÃ­sticas TÃ©cnicas\n",
    "- **Canales activos:**  \n",
    "  - F1â€“F16 â†’ antebrazo (2 anillos Ã— 8 electrodos).  \n",
    "  - W1â€“W12 â†’ muÃ±eca (2 anillos Ã— 6 electrodos).  \n",
    "  - U1â€“U4 â†’ canales no usados (espaciadores).  \n",
    "- **TamaÃ±o de cada registro:** `10240 x 32` (5 s Ã— 2048 Hz Ã— 32 canales).  \n",
    "- **Archivos extra:**  \n",
    "  - `grabmyo_convert_wfdb_to_mat.py` â†’ conversiÃ³n a MATLAB.  \n",
    "  - `grabmyo_feature_extraction.m` â†’ extracciÃ³n de caracterÃ­sticas.  \n",
    "  - `grabmyo_visualize.py` â†’ visualizaciÃ³n de datos.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Posibles Usos\n",
    "- ğŸ›¡ï¸ **AutenticaciÃ³n biomÃ©trica** â†’ EMG como contraseÃ±a o firma muscular.  \n",
    "- ğŸ†” **IdentificaciÃ³n** â†’ reconocer al usuario entre N sujetos.  \n",
    "- ğŸ¦¾ **Reconocimiento de gestos** â†’ aplicaciones en rehabilitaciÃ³n, prÃ³tesis y control de interfaces.  \n",
    "- ğŸ”„ **AdaptaciÃ³n a electrodos desplazados** â†’ tÃ©cnicas de transferencia y robustez.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a3958",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£2ï¸âƒ£ [sEMG for Basic Hand Movements â€“ UCI Repository](https://archive.ics.uci.edu/dataset/313/semg+for+basic+hand+movements ) ğŸ“š\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ InformaciÃ³n General\n",
    "- **Nombre completo:** *sEMG for Basic Hand movements*  \n",
    "- **Autores:** Christos Sapsanis, Anthony Tzes, G. Georgoulas  \n",
    "- **Publicado en:** UCI Machine Learning Repository (2014)  \n",
    "- **DOI:** [10.24432/C5TK53](https://doi.org/10.24432/C5TK53)  \n",
    "- **Sujetos:**  \n",
    "  - **Database 1:** 5 sujetos (2 hombres, 3 mujeres, 20â€“22 aÃ±os).  \n",
    "  - **Database 2:** 1 sujeto (hombre, 22 aÃ±os) durante 3 dÃ­as consecutivos.  \n",
    "- **Movimientos:** 6 tipos de agarre manual + reposo.  \n",
    "- **Frecuencia de muestreo:** 500 Hz.  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41597-022-01836-y/MediaObjects/41597_2022_1836_Fig1_HTML.png\" alt=\"semgrefernce2\" height=\"400\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ PropÃ³sito del Dataset\n",
    "El dataset fue diseÃ±ado para:  \n",
    "1. ğŸ§  **ClasificaciÃ³n de movimientos de la mano** a partir de EMG superficial.  \n",
    "2. ğŸ’ª **AnÃ¡lisis de activaciÃ³n muscular** en movimientos funcionales.  \n",
    "3. ğŸ¦¾ **Aplicaciones biomÃ©dicas** en rehabilitaciÃ³n, prÃ³tesis e interfaces hombre-mÃ¡quina.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© DiseÃ±o Experimental\n",
    "- **Electrodos utilizados:**  \n",
    "  - 2 canales â†’ *Flexor Carpi Ulnaris* y *Extensor Carpi Radialis (longus y brevis)*.  \n",
    "  - Referencia en el antebrazo, fijados con bandas elÃ¡sticas.  \n",
    "- **Sistema de adquisiciÃ³n:**  \n",
    "  - Delsys Bagnoliâ„¢ 2-channel EMG System.  \n",
    "  - NI USB-009 para conversiÃ³n A/D.  \n",
    "- **Filtrado de la seÃ±al:**  \n",
    "  - **Band-pass Butterworth:** 15â€“500 Hz.  \n",
    "  - **Notch:** 50 Hz para eliminar ruido de lÃ­nea.  \n",
    "- **Protocolo:**  \n",
    "  - Cada gesto repetido mÃºltiples veces (30 o 100, segÃºn base).  \n",
    "  - DuraciÃ³n de las pruebas: 5â€“6 s.  \n",
    "  - Los sujetos ajustaban fuerza y velocidad libremente.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ–ï¸ Movimientos Registrados\n",
    "Los 6 gestos corresponden a agarres cotidianos:  \n",
    "1. **Spherical** â†’ agarrar objetos redondos.  \n",
    "2. **Tip** â†’ sujetar objetos pequeÃ±os.  \n",
    "3. **Palmar** â†’ agarrar con palma abierta.  \n",
    "4. **Lateral** â†’ sujetar objetos planos/finos.  \n",
    "5. **Cylindrical** â†’ sostener cilindros.  \n",
    "6. **Hook** â†’ cargar objetos pesados en forma de gancho.  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.researchgate.net/publication/357759649/figure/fig1/AS:11431281122325537@1677270339787/Gestures-performed-in-sEMG-for-Basic-Hand-movements-Data-Set-3.jpg\" alt=\"semgrefernce3\" height=\"300\">\n",
    "</p>\n",
    "---\n",
    "\n",
    "## ğŸ“Š CaracterÃ­sticas del Dataset\n",
    "- **Database 1 (5 sujetos):**  \n",
    "  - 6 gestos Ã— 30 repeticiones Ã— 6 s.  \n",
    "  - Archivos `.mat` con **12 matrices por sujeto** (2 canales Ã— 6 gestos).  \n",
    "  - Cada matriz: **30 filas Ã— 3000 columnas** (seÃ±al en voltaje).  \n",
    "\n",
    "- **Database 2 (1 sujeto, 3 dÃ­as):**  \n",
    "  - 6 gestos Ã— 100 repeticiones Ã— 5 s.  \n",
    "  - Archivos `.mat` por dÃ­a.  \n",
    "  - Cada matriz: **100 filas Ã— 2500 columnas**.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Posibles Usos\n",
    "- ğŸ“ˆ **ClasificaciÃ³n multiclase** â†’ modelos ML para reconocer gestos.  \n",
    "- ğŸ¦¾ **Control de prÃ³tesis** y dispositivos de asistencia.  \n",
    "- ğŸ‹ï¸ **Estudio de variabilidad** â†’ entre sujetos y entre dÃ­as.  \n",
    "- ğŸ›¡ï¸ **Interfaces biomÃ©dicas** â†’ rehabilitaciÃ³n y neuroingenierÃ­a.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d726b",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£3ï¸âƒ£ [Dataset: EMG Data for Gestures](https://archive.ics.uci.edu/dataset/481/emg+data+for+gestures ) ğŸ“š\n",
    "\n",
    "ğŸ“Œ **DonaciÃ³n:** 06/01/2019  \n",
    "ğŸ“Œ **Repositorio:** UCI Machine Learning Repository  \n",
    "ğŸ“Œ **DOI:** [10.24432/C5ZP5C](https://doi.org/10.24432/C5ZP5C)  \n",
    "ğŸ“Œ **Licencia:** Creative Commons Attribution 4.0 International (CC BY 4.0)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” DescripciÃ³n general  \n",
    "Este dataset contiene seÃ±ales **EMG crudas** registradas mediante un brazalete **Myo Thalmic**, el cual cuenta con **8 sensores** dispuestos alrededor del antebrazo.  \n",
    "\n",
    "- **Sujetos:** 36 voluntarios  \n",
    "- **Gestos estÃ¡ticos registrados:** 6â€“7 tipos  \n",
    "- **DuraciÃ³n:** Cada gesto fue sostenido **3 segundos**, con **3 segundos de pausa** entre gestos.  \n",
    "- **NÃºmero de instancias:** entre **40,000â€“50,000 registros por archivo** (garantizados al menos 30,000).  \n",
    "- **Tareas posibles:** clasificaciÃ³n de gestos, procesamiento de seÃ±ales biomÃ©dicas, biometrÃ­a.  \n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ InstrumentaciÃ³n y protocolo  \n",
    "- **Dispositivo:** Myo Thalmic Bracelet  \n",
    "- **Sensores:** 8 canales EMG distribuidos en el antebrazo  \n",
    "- **ConexiÃ³n:** Bluetooth a PC  \n",
    "- **Datos adquiridos:** seÃ±ales EMG crudas (sin filtrado previo)  \n",
    "\n",
    "Cada sujeto realizÃ³ **2 series de gestos**:  \n",
    "1. Mano en reposo  \n",
    "2. Mano cerrada en puÃ±o  \n",
    "3. FlexiÃ³n de muÃ±eca  \n",
    "4. ExtensiÃ³n de muÃ±eca  \n",
    "5. DesviaciÃ³n radial  \n",
    "6. DesviaciÃ³n cubital  \n",
    "7. Palma extendida (*no todos los sujetos la realizaron*)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‚ Estructura de los archivos  \n",
    "Cada archivo de datos contiene **10 columnas**:  \n",
    "\n",
    "1. **Tiempo (ms)**  \n",
    "2â€“9. **Canales EMG** (8 sensores del brazalete)  \n",
    "10. **Etiqueta (gesto):**  \n",
    "   - `0` â†’ sin marcar  \n",
    "   - `1` â†’ mano en reposo  \n",
    "   - `2` â†’ puÃ±o cerrado  \n",
    "   - `3` â†’ flexiÃ³n de muÃ±eca  \n",
    "   - `4` â†’ extensiÃ³n de muÃ±eca  \n",
    "   - `5` â†’ desviaciÃ³n radial  \n",
    "   - `6` â†’ desviaciÃ³n cubital  \n",
    "   - `7` â†’ palma extendida  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š CaracterÃ­sticas del dataset  \n",
    "- **Tipo:** Series temporales  \n",
    "- **Ãrea temÃ¡tica:** Salud y medicina  \n",
    "- **Tarea asociada:** ClasificaciÃ³n  \n",
    "- **Formato de datos:** Texto plano (.txt)  \n",
    "- **Valores faltantes:** No presenta  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‚ Archivos disponibles (ejemplos)  \n",
    "- `1_raw_data_13-11_18.03.16.txt` (4.4 MB)  \n",
    "- `1_raw_data_10-51_07.04.16.txt` (4.5 MB)  \n",
    "- `2_raw_data_13-29_21.03.16.txt` (4.6 MB)  \n",
    "*(73 archivos en total, organizados por sujeto y sesiÃ³n)*  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘©â€ğŸ”¬ Autores  \n",
    "- N. Krilova  \n",
    "- I. Kastalskiy  \n",
    "- V. Kazantsev  \n",
    "- V.A. Makarov  \n",
    "- S. Lobov  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Uso en Python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el dataset\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Cargar datos\n",
    "emg_data_for_gestures = fetch_ucirepo(id=481)\n",
    "\n",
    "# SeÃ±ales y etiquetas\n",
    "X = emg_data_for_gestures.data.features\n",
    "y = emg_data_for_gestures.data.targets\n",
    "\n",
    "# Metadatos\n",
    "print(emg_data_for_gestures.metadata)\n",
    "\n",
    "# Variables\n",
    "print(emg_data_for_gestures.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef750a85",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£4ï¸âƒ£[DB1: Ninapro EMG & Kinematic Data for Hand Movements](https://ninapro.hevs.ch/instructions/DB1.html) ğŸ“š\n",
    "ğŸ“Œ **Repositorio:** Ninapro (Swiss National Science Foundation)  \n",
    "---\n",
    "\n",
    "##  ğŸ” DescripciÃ³n general  \n",
    "Este dataset incluye seÃ±ales **sEMG** y datos **cinemÃ¡ticos** registrados de **27 sujetos intactos**, repitiendo **52 movimientos de mano** mÃ¡s posiciÃ³n de reposo. :contentReference[oaicite:0]{index=0}  \n",
    "- **Sujetos:** 27 personas (23 diestros derechos y 4 zurdos), edades entre aprox. 22 y 40 aÃ±os :contentReference[oaicite:1]{index=1}  \n",
    "- **Movimientos:** 52 gestos distintos de mano, ademÃ¡s de reposo, organizados en tres ejercicios:  \n",
    "  1. Movimientos bÃ¡sicos de los dedos  \n",
    "  2. Configuraciones isomÃ©tricas, isotÃ³nicas del puÃ±o y movimientos de muÃ±eca  \n",
    "  3. Agarres y movimientos funcionales :contentReference[oaicite:2]{index=2}  \n",
    "- **Repeticiones:** 10 repeticiones por gesto :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "---\n",
    "\n",
    "##  âš™ï¸ InstrumentaciÃ³n y protocolo  \n",
    "- **Dispositivos:**  \n",
    "  - 10 electrodos sEMG â€œOtto Bock MyoBock 13E200â€ (8 colocados alrededor del antebrazo, mÃ¡s dos sobre flexor y extensor digitorum superficialis) :contentReference[oaicite:4]{index=4}  \n",
    "  - CyberGlove II para captura cinemÃ¡tica (22 sensores de Ã¡ngulo sin calibraciÃ³n) :contentReference[oaicite:5]{index=5}  \n",
    "- **Protocolo de adquisiciÃ³n:** Los sujetos repitieron los movimientos guiados mediante videos en laptop, siguiendo las tres fases descritas arriba :contentReference[oaicite:6]{index=6}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ—‚ Estructura de los archivos  \n",
    "Cada sujeto y ejercicio cuenta con un archivo **MATLAB (.mat)** que incluye variables sincronizadas:  \n",
    "- `Emg` (10 columnas): seÃ±ales sEMG (8 alrededor + 2 del flexor/extensor)  \n",
    "- `Glove` (22 columnas): seÃ±ales sin calibrar del CyberGlove  \n",
    "- `Stimulus`: etiqueta del movimiento mostrado  \n",
    "- `Restimulus`: etiqueta refinada post-adquisiciÃ³n  \n",
    "- `Repetition`: nÃºmero de repeticiÃ³n del estÃ­mulo  \n",
    "- `Rerepetition`: repeticiÃ³n del restimulus :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ“Š CaracterÃ­sticas del dataset  \n",
    "- **Tipo:** Series temporales sin procesar  \n",
    "- **Ãrea:** Control de prÃ³tesis, biomecÃ¡nica, biometrÃ­a  \n",
    "- **Tareas:** ClasificaciÃ³n de gestos, modelado de movimiento  \n",
    "- **Formato:** Archivos `.mat` (MATLAB), posible conversiÃ³n a otros formatos  \n",
    "- **Valores faltantes:** No se reportan :contentReference[oaicite:8]{index=8}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ“‚ Archivos disponibles  \n",
    "- Para cada sujeto (1 a 27), un archivo `.zip` (p. ej., `s1.zip`, `s2.zip`, ..., `s27.zip`) con los datos correspondientes :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ‘©â€ğŸ”¬ Autores / Referencias  \n",
    "- **Publicaciones asociadas:**  \n",
    "  - Atzori et al., *Characterization of a Benchmark Database for Myoelectric Movement Classification*, IEEE T-NSRE, 2014  \n",
    "  - *Electromyography data for non-invasive naturally-controlled robotic hand prostheses*, Scientific Data, 2014  \n",
    "  - *Building the NINAPRO Database: A Resource for the Biorobotics Community* :contentReference[oaicite:10]{index=10}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ›  Uso en Python (ejemplo bÃ¡sico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058644b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Cargar archivo .mat de un sujeto / ejercicio\n",
    "data = scipy.io.loadmat('s1_ex1.mat')\n",
    "\n",
    "emg = data['Emg']           # (N_samples, 10)\n",
    "glove = data['Glove']       # (N_samples, 22)\n",
    "stim = data['Stimulus'].flatten()\n",
    "restim = data['Restimulus'].flatten()\n",
    "rep = data['Repetition'].flatten()\n",
    "\n",
    "print(\"EMG shape:\", emg.shape)\n",
    "print(\"First 5 Stimuli:\", stim[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e54e5e",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£5ï¸âƒ£[EMG data of stroke and healthy subjects when performing hand-to-nose movement](https://figshare.com/articles/dataset/) ğŸ“š\n",
    "\n",
    "ğŸ“Œ **DonaciÃ³n / PublicaciÃ³n:** Aproximadamente hace 3.6 aÃ±os (~2021) :contentReference[oaicite:0]{index=0}  \n",
    "ğŸ“Œ **Repositorio:** Figshare (Kunkun Zhao) :contentReference[oaicite:1]{index=1}  \n",
    "ğŸ“Œ **Licencia:** No especificada en la fuente pÃºblica  \n",
    "ğŸ“Œ **DOI:** No se menciona un DOI especÃ­fico en la descripciÃ³n :contentReference[oaicite:2]{index=2}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ” DescripciÃ³n general  \n",
    "Este dataset contiene seÃ±ales **EMG en bruto** registradas durante el movimiento â€œmano a la narizâ€, tanto de **pacientes con accidente cerebrovascular (stroke)** como de **sujetos sanos**. Incluye tambiÃ©n puntuaciones funcionales Fugl-Meyer (FM) de 20 pacientes con stroke.  \n",
    "- **Archivos incluidos** (3 archivos):  \n",
    "  - `FM.mat`: puntuaciones funcionales de miembros superiores (Fugl-Meyer) para 20 pacientes con stroke.  \n",
    "  - `rawEMGData_H.mat`: seÃ±ales EMG de sujetos sanos.  \n",
    "  - `rawEMGData_P.mat`: seÃ±ales EMG de pacientes con stroke. :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "---\n",
    "\n",
    "##  âš™ï¸ InstrumentaciÃ³n y protocolo  \n",
    "- **Dispositivos / configuraciÃ³n:** No se especifican los detalles del equipo utilizado (tipo de sensores, ubicaciÃ³n de electrodos, frecuencia de muestreo, etc.) en la fuente consultada.  \n",
    "- **Protocolo:** Se registra el movimiento mano-a-nariz en participantes sanos y pacientes post-stroke, aunque el protocolo exacto (repeticiones, duraciÃ³n, condiciones estÃ¡ndar) no se detalla en la descripciÃ³n pÃºblica.  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ—‚ Estructura de los archivos  \n",
    "Los archivos `.mat` incluidos contienen lo siguiente:  \n",
    "- `FM.mat`: puntuaciones Fugl-Meyer para la funciÃ³n motora de 20 pacientes con stroke.  \n",
    "- `rawEMGData_H.mat` y `rawEMGData_P.mat`: seÃ±ales EMG en bruto de los sujetos sanos (`H`) y de los pacientes con stroke (`P`). :contentReference[oaicite:4]{index=4}  \n",
    "\n",
    "No se detalla la estructura interna (nÃºmero de canales, etiquetas, formato temporal), ya que la fuente no proporciona informaciÃ³n especÃ­fica al respecto.\n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ“Š CaracterÃ­sticas del dataset  \n",
    "- **Tipo:** Series temporales (EMG crudo)  \n",
    "- **Ãrea temÃ¡tica:** RehabilitaciÃ³n, neurologÃ­a, biomecÃ¡nica  \n",
    "- **Tarea asociada:** AnÃ¡lisis de seÃ±ales EMG y posible clasificaciÃ³n o correlaciÃ³n con funciÃ³n motora (Fugl-Meyer)  \n",
    "- **Formato:** Archivos `.mat` (MATLAB)  \n",
    "- **Valores faltantes:** No se especifica si hay valores faltantes o no :contentReference[oaicite:5]{index=5}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ“‚ Archivos disponibles  \n",
    "- `FM.mat` â€“ puntuaciones Fugl-Meyer para 20 pacientes con accidente cerebrovascular  \n",
    "- `rawEMGData_H.mat` â€“ datos EMG de sujetos sanos  \n",
    "- `rawEMGData_P.mat` â€“ datos EMG de pacientes con stroke :contentReference[oaicite:6]{index=6}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ‘©â€ğŸ”¬ Autores / Referencias  \n",
    "- **Autor(es):** Kunkun Zhao y colaboradores (segÃºn figshare) :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ›  Uso en Python (ejemplo bÃ¡sico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb392e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos EMG de sujetos sanos\n",
    "data_h = scipy.io.loadmat('rawEMGData_H.mat')\n",
    "# Cargar datos EMG de pacientes con stroke\n",
    "data_p = scipy.io.loadmat('rawEMGData_P.mat')\n",
    "# Cargar puntuaciones Fugl-Meyer\n",
    "fm = scipy.io.loadmat('FM.mat')\n",
    "\n",
    "# Ejemplo de exploraciÃ³n bÃ¡sica\n",
    "print(\"Claves disponibles en rawEMGData_H:\", data_h.keys())\n",
    "print(\"Claves disponibles en rawEMGData_P:\", data_p.keys())\n",
    "print(\"Claves disponibles en FM.mat:\", fm.keys())\n",
    "\n",
    "# Si hay una variable clave 'emg', podrÃ­amos hacer:\n",
    "# emg_h = data_h['emg']\n",
    "# print(\"Shape EMG sujetos sanos:\", emg_h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dd9be",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£6ï¸âƒ£ [EMG datasets for hand-reaching movements for multiple directions of healthy and post-stroke individuals](https://data.mendeley.com/datasets/f4hh43nd78/1)  ğŸ“š \n",
    "\n",
    "ğŸ“Œ **DonaciÃ³n / PublicaciÃ³n:** 5 de junio de 2018 (VersiÃ³n 1)  \n",
    "ğŸ“Œ **Repositorio:** Mendeley Data (Universidad de Haifa)  \n",
    "ğŸ“Œ **Licencia:** Creative Commons Attribution 4.0 International (CC BY 4.0) :contentReference[oaicite:0]{index=0}  \n",
    "ğŸ“Œ **DOI:** 10.17632/f4hh43nd78.1 :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ” DescripciÃ³n general  \n",
    "Este dataset contiene seÃ±ales **EMG multi-canal** registradas durante movimientos de alcance manual en **individuos sanos y post-stroke**. Los participantes realizaron movimientos de alcance hacia **9 direcciones objetivo** bajo monitoreo de sensores EMG. :contentReference[oaicite:2]{index=2}  \n",
    "- **Participantes:** Carpeta â€œHealthyâ€ con 12 sujetos sanos y carpeta â€œPost-strokeâ€ con 13 sujetos. :contentReference[oaicite:3]{index=3}  \n",
    "- **Movimientos:** Alcance manual hacia 9 direcciones consistentes entre todos los sujetos. :contentReference[oaicite:4]{index=4}  \n",
    "\n",
    "---\n",
    "\n",
    "##  âš™ï¸ InstrumentaciÃ³n y protocolo  \n",
    "- **Sensores:** 8 electrodos de superficie EMG colocados en 8 mÃºsculos del hombro y brazo. :contentReference[oaicite:5]{index=5}  \n",
    "- **Protocolo:**  \n",
    "  1. Mediciones de **MÃ¡xima ContracciÃ³n Voluntaria (MVC)** de los mÃºsculos (8 archivos por sujeto).  \n",
    "  2. Archivos denominados `Target_XXX` para cada direcciÃ³n de alcance (9 archivos para sujetos sanos, 6 para algunos post-stroke). :contentReference[oaicite:6]{index=6}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ—‚ Estructura de los archivos  \n",
    "En cada carpeta individual (por sujeto) se encuentran:  \n",
    "- 8 archivos MVC (MÃ¡xima ContracciÃ³n Voluntaria).  \n",
    "- Archivos `Target_XXX`, cada uno con seÃ±ales de los 8 mÃºsculos durante el movimiento hacia una direcciÃ³n especÃ­fica. :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ“Š CaracterÃ­sticas del dataset  \n",
    "- **Tipo:** Series temporales EMG sin procesar.  \n",
    "- **Ãrea temÃ¡tica:** RehabilitaciÃ³n, accidente cerebrovascular (stroke), control motor. :contentReference[oaicite:8]{index=8}  \n",
    "- **Tareas asociadas:** ComparaciÃ³n entre grupos (sanos vs post-stroke), anÃ¡lisis de fuerza muscular, clasificaciÃ³n de direcciones de movimiento.  \n",
    "- **Formato:** Archivos (tipo no especificado en la descripciÃ³n, presumiblemente binarios legibles o formatos de texto estructurado).  \n",
    "- **Valores faltantes:** No se indica su existencia en la fuente consultada.  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ“‚ Archivos disponibles  \n",
    "- **Para cada sujeto sano (12):**  \n",
    "  - 8 archivos MVC  \n",
    "  - 9 archivos `Target_XXX` (direcciones de alcance)  \n",
    "- **Para cada sujeto post-stroke (13):**  \n",
    "  - 8 archivos MVC  \n",
    "  - 6 archivos `Target_XXX`, segÃºn disponibilidad de direcciones. :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ‘©â€ğŸ”¬ Autores / Referencias  \n",
    "- **Contribuyente principal:** Sharon Israely (Universidad de Haifa) :contentReference[oaicite:10]{index=10}\n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ›  Uso en Python (ejemplo bÃ¡sico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c270ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ejemplo: listar archivos de un sujeto\n",
    "subject_folder = 'Healthy/subject01'  # ajustar ruta local\n",
    "files = os.listdir(subject_folder)\n",
    "print(\"Archivos disponibles:\", files)\n",
    "\n",
    "# Cargar un archivo (si es formato texto o numpy)\n",
    "# Ajusta segÃºn formato real, a modo de ejemplo:\n",
    "# emg_data = np.loadtxt(os.path.join(subject_folder, files[0]))\n",
    "# print(\"Shape de emg_data:\", emg_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a9ea1",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£7ï¸âƒ£ [Post-stroke hand gesture recognition via one-shot transfer learning using prototypical networks](https://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-024-01398-7)  ğŸ“š\n",
    "\n",
    "ğŸ“Œ **PublicaciÃ³n:** 12 de junio de 2024 :contentReference[oaicite:0]{index=0}  \n",
    "ğŸ“Œ **Repositorio / Fuente:** Journal of NeuroEngineering and Rehabilitation (Open Access, BMC / Springer Nature) :contentReference[oaicite:1]{index=1}  \n",
    "ğŸ“Œ **Licencia:** Creative Commons Attribution 4.0 International (CC BY 4.0) â€“ datos abiertos incluidos :contentReference[oaicite:2]{index=2}  \n",
    "ğŸ“Œ **DOI:** 10.1186/s12984-024-01398-7 :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ” DescripciÃ³n general  \n",
    "Este estudio aborda el reconocimiento de gestos manuales en pacientes post-stroke mediante aprendizaje de transferencia de un solo ejemplo (one-shot transfer learning), utilizando **prototypical networks** â€”un tipo de red neuronal entrenada para distinguir gestos con muy pocos ejemplos.\n",
    "\n",
    "- **Sujetos participantes:** 20 personas con antecedente de accidente cerebrovascular (stroke), reclutados en el Hospital Huashan, ShanghÃ¡i :contentReference[oaicite:4]{index=4}  \n",
    "- **Gestos evaluados:** 7 movimientos relevantes para actividades diarias:\n",
    "  1. FlexiÃ³n masiva  \n",
    "  2. ExtensiÃ³n masiva  \n",
    "  3. FlexiÃ³n palmar de muÃ±eca (wrist volar flexion)  \n",
    "  4. DorsiflexiÃ³n de muÃ±eca (wrist dorsiflexion)  \n",
    "  5. PronaciÃ³n de antebrazo  \n",
    "  6. SupinaciÃ³n de antebrazo  \n",
    "  7. Reposo (rest) :contentReference[oaicite:5]{index=5}  \n",
    "\n",
    "---\n",
    "\n",
    "##  âš™ï¸ InstrumentaciÃ³n y protocolo  \n",
    "- Los participantes usaron sensores **EMG** en el antebrazo, sensado **FMG** (forza muscular) y **IMU** (unidad de mediciÃ³n inercial) en la muÃ±eca mientras realizaban los gestos mencionados :contentReference[oaicite:6]{index=6}  \n",
    "- Se diseÃ±Ã³ un modelo basado en prototypical networks, junto con selecciÃ³n de caracterÃ­sticas mediante **K-Best** y ajuste de la ventana temporal para mejorar la precisiÃ³n :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ—‚ Estructura de datos / Archivos  \n",
    "- El artÃ­culo no detalla almacenamiento pÃºblico de conjuntos de datos ni la estructura interna (variables, formatos especÃ­ficos). Sin embargo, se menciona que el cÃ³digo se encuentra disponible en GitHub:\n",
    "  - Repositorio: `https://github.com/HSarwat/Few-Shot-Proto-TL.git` :contentReference[oaicite:8]{index=8}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ“Š CaracterÃ­sticas clave  \n",
    "- **Tipo de datos:** SeÃ±ales biomÃ©tricas (EMG, FMG, IMU), series temporales  \n",
    "- **Ãrea temÃ¡tica:** RehabilitaciÃ³n motora post-stroke, inteligencia artificial, control de gestos  \n",
    "- **Tarea asociada:** Reconocimiento de gestos con aprendizaje de transferencia minimalista (one-shot)  \n",
    "- **Modelos comparados:** Prototypical networks vs. redes neuronales tradicionales, LGBM, LDA, SVM; ademÃ¡s, enfoques dependientes e independientes del sujeto :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ Resultados principales  \n",
    "- **PrecisiÃ³n alcanzada:** 82.2 % en clasificaciÃ³n de gestos con el modelo propuesto  \n",
    "- **Comparativa:**  \n",
    "  - Redes neuronales one-shot TL: 63.17 %  \n",
    "  - Redes neuronales tradicionales: 59.72 %  \n",
    "  - LGBM: 65.09 %  \n",
    "  - LDA: 63.35 %  \n",
    "  - SVM: 54.5 % :contentReference[oaicite:10]{index=10}  \n",
    "- El modelo propuesto se acercÃ³ al rendimiento de clasificadores dependientes del sujeto, sÃ³lo ligeramente por debajo del SVM (83.84 %) y superior a NN (81.62 %), LGBM (80.79 %) y LDA (74.89 %) :contentReference[oaicite:11]{index=11}  \n",
    "- **K-Best features:** Mejoraron la precisiÃ³n en 3 de 6 clasificadores  \n",
    "- **Ventana mÃ¡s amplia:** MejorÃ³ el rendimiento en todos los clasificadores con un aumento promedio de precisiÃ³n del 4.28 % :contentReference[oaicite:12]{index=12}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ‘©â€ğŸ”¬ Autores / Referencias  \n",
    "- **Autores principales:** Hussein Sarwat, Amr Alkhashab, Xinyu Song, Shuo Jiang, Jie Jia, Peter B. Shull :contentReference[oaicite:13]{index=13}  \n",
    "- **Registro del estudio:** CHiCTR1800017568 (Chinese Clinical Trial Registry), registro del 4 de agosto de 2018 :contentReference[oaicite:14]{index=14}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ›  Uso en Python (ejemplo hipotÃ©tico)\n",
    "El repositorio de GitHub mencionado puede contener datos de ejemplo y scripts. Un posible paso inicial en Python podrÃ­a ser:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee387aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prototypical_network import ProtoNet  # ejemplo hipotÃ©tico\n",
    "from data_loader import load_data  # funciÃ³n ficticia\n",
    "\n",
    "# Cargar datos â€” supongamos que load_data retorna (X_train, y_train), (X_test, y_test)\n",
    "(X_train, y_train), (X_test, y_test) = load_data('ruta_al_dataset')\n",
    "\n",
    "model = ProtoNet(input_dim=X_train.shape[1], num_classes=7)  # segÃºn teclas del dataset\n",
    "model.train_one_shot(X_train, y_train)\n",
    "\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"One-shot ProtoNet accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c14b16",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£8ï¸âƒ£ [A Novel Bilateral Data Fusion Approach for EMG-Driven Deep Learning in Post-Stroke Paretic Gesture Recognition](https://pubmed.ncbi.nlm.nih.gov/40573553/)  ğŸ“š\n",
    "\n",
    "ğŸ“Œ **PublicaciÃ³n:** 11 de junio de 2025 â€“ *Sensors (Basel)* :contentReference[oaicite:0]{index=0}  \n",
    "ğŸ“Œ **Repositorio / Fuente:** Journal *Sensors* (MDPI), acceso abierto (PMC disponible) :contentReference[oaicite:1]{index=1}  \n",
    "ğŸ“Œ **Licencia:** Acceso abierto bajo *Creative Commons* (implÃ­cito por polÃ­tica de MDPI) :contentReference[oaicite:2]{index=2}  \n",
    "ğŸ“Œ **DOI:** 10.3390/s25123664 :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ” DescripciÃ³n general  \n",
    "Este estudio presenta un modelo hÃ­brido de aprendizaje profundo (CNN-LSTM) para reconocer gestos manuales desde seÃ±ales **EMG superficiales** en pacientes subagudos post-stroke. Se introdujo una metodologÃ­a novedosa de **fusiÃ³n bilateral** que incorpora tambiÃ©n seÃ±ales del miembro no afectado para mejorar el entrenamiento. :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "- **Participantes:** 25 pacientes post-stroke (fase subaguda), con dos sesiones de captura espaciadas por al menos una semana :contentReference[oaicite:5]{index=5}  \n",
    "- **Movimientos registrados (7 gestos):**  \n",
    "  1. Reposo (rest)  \n",
    "  2. PuÃ±o cerrado (clenched fist)  \n",
    "  3. Pinza Ã­ndice (index pinch)  \n",
    "  4. FlexiÃ³n de muÃ±eca (wrist flexion)  \n",
    "  5. ExtensiÃ³n de muÃ±eca (wrist extension)  \n",
    "  6. Mano abierta (spread hand)  \n",
    "  7. Pulgar arriba (thumbs up) :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "---\n",
    "\n",
    "##  âš™ï¸ InstrumentaciÃ³n y protocolo  \n",
    "- **Electrodos sEMG** bipolares colocados en cuatro regiones del antebrazo:  \n",
    "  - Flexor carpi radialis  \n",
    "  - Flexor carpi ulnaris  \n",
    "  - RegiÃ³n tenar (abductor pollicis brevis & flexor pollicis brevis)  \n",
    "  - Extensor digitorum communis :contentReference[oaicite:7]{index=7}  \n",
    "- **Grabaciones:** Cada paciente realizÃ³ al menos 10 intentos por gesto, en ambos miembros (afectado y no afectado). La fusiÃ³n bilateral se aplicÃ³ durante el entrenamiento; las pruebas se realizaron solo con datos del miembro afectado. Se validÃ³ mediante validaciÃ³n cruzada de 10 pliegues, repetida 100 veces :contentReference[oaicite:8]{index=8}\n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ—‚ Estructura de los datos  \n",
    "El artÃ­culo sÃ­ describe la metodologÃ­a y arquitectura, pero **no publica el dataset** ni detalla el formato de los archivos (no disponible pÃºblicamente). :contentReference[oaicite:9]{index=9}\n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ“Š Resultados principales  \n",
    "- **PrecisiÃ³n del modelo CNN-LSTM hÃ­brido:**  \n",
    "  - *Dataset A:* entre 82.27 % y 85.66 %  \n",
    "  - *Dataset B:* entre 81.69 % y 88.36 % :contentReference[oaicite:10]{index=10}  \n",
    "- **Incrementos con fusiÃ³n bilateral (especialmente en la subtarea de 3 gestos):**  \n",
    "  - *Dataset A:* de 73.01 % a 78.42 %  \n",
    "  - *Dataset B:* de 77.95 % a 85.69 % :contentReference[oaicite:11]{index=11}  \n",
    "- Se reportan mejoras en precisiÃ³n, sensibilidad, especificidad y F1-score gracias a la estrategia propuesta :contentReference[oaicite:12]{index=12}\n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ‘©â€ğŸ”¬ Autores / Afilaciones  \n",
    "- **Autores principales:** Alexey Anastasiev, Hideki Kadone, Aiki Marushima, Hiroki Watanabe, Alexander Zaboronok, Shinya Watanabe, Akira Matsumura, Kenji Suzuki, Yuji Matsumaru, Hiroyuki Nishiyama, Eiichi Ishikawa :contentReference[oaicite:13]{index=13}  \n",
    "- **Afiliaciones principales:** Universidad de Tsukuba (JapÃ³n), incluyendo el Hospital Universitario de Tsukuba, el Centro de InvestigaciÃ³n Cybernics, y otros centros asociados :contentReference[oaicite:14]{index=14}\n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ›  Uso en Python (ejemplo hipotÃ©tico)  \n",
    "Aunque el dataset no estÃ¡ disponible, este es un ejemplo de cÃ³mo podrÃ­a estructurarse un entrenamiento similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d40ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo teÃ³rico: dataset preparado como arrays NumPy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# SupÃ³n que tienes los siguientes arrays:\n",
    "# X_paretic: datos EMG del miembro afectado (n_samples, timesteps, channels)\n",
    "# X_nonparetic: datos EMG del miembro no afectado\n",
    "# y_labels: etiquetas de los gestos (0â€“6)\n",
    "\n",
    "# FusiÃ³n bilateral: concatenar ambos conjuntos para entrenamiento\n",
    "X_train = np.concatenate([X_paretic, X_nonparetic], axis=0)\n",
    "y_train = np.concatenate([y_labels, y_labels], axis=0)  # mismas etiquetas\n",
    "\n",
    "# Arquitectura CNN-LSTM simplificada\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(None, X_train.shape[2])),\n",
    "    layers.MaxPooling1D(2),\n",
    "    layers.LSTM(128),\n",
    "    layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento (hipotÃ©tico)\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# EvaluaciÃ³n solo con datos del miembro afectado\n",
    "loss, accuracy = model.evaluate(X_paretic, y_labels)\n",
    "print(f\"PrecisiÃ³n en miembro afectado: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd659c",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£9ï¸âƒ£ [Decoding hand and wrist movement intention from chronic stroke survivors with hemiparesis using a user-friendly, wearable EMG-based neural interface](https://pubmed.ncbi.nlm.nih.gov/38218901/)  ğŸ“š\n",
    "\n",
    "ğŸ“Œ **PublicaciÃ³n:** 13 de enero de 2024 â€“ *Journal of NeuroEngineering and Rehabilitation* (Vol. 21, ArtÃ­culo 7) :contentReference[oaicite:0]{index=0}  \n",
    "ğŸ“Œ **Repositorio / Fuente:** Journal of NeuroEngineering and Rehabilitation (Open Access, BMC / Springer Nature) :contentReference[oaicite:1]{index=1}  \n",
    "ğŸ“Œ **Licencia:** Acceso abierto bajo *Creative Commons* (implÃ­cito por polÃ­tica de BMC) :contentReference[oaicite:2]{index=2}  \n",
    "ğŸ“Œ **DOI:** 10.1186/s12984-023-01301-w :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ” DescripciÃ³n general  \n",
    "El estudio presenta el **sistema NeuroLifeÂ® EMG**, una manga portÃ¡til con hasta **150 electrodos integrados** para registrar seÃ±ales de **EMG de alta densidad (HDEMG)**. Se utilizÃ³ para decodificar la intenciÃ³n de movimiento del antebrazo, muÃ±eca y mano en **personas con hemiparesia crÃ³nica post-stroke**, destacÃ¡ndose tambiÃ©n por ser cÃ³modo, portable y de configuraciÃ³n simple para uso domÃ©stico. :contentReference[oaicite:4]{index=4}  \n",
    "\n",
    "- **Participantes:**  \n",
    "  - 7 pacientes crÃ³nicos post-stroke (hemiparesia) â€“ edad promedio ~60 aÃ±os.  \n",
    "  - 7 participantes sanos (edad promedio ~27 aÃ±os), como grupo control. :contentReference[oaicite:5]{index=5}  \n",
    "\n",
    "---\n",
    "\n",
    "##  âš™ï¸ InstrumentaciÃ³n y protocolo  \n",
    "- **Dispositivo:** Manga translÃºcida con hasta 150 electrodos embebidos, fÃ¡cilmente colocable y conectado a un mÃ³dulo de adquisiciÃ³n (Intan), ideal para uso en el hogar. :contentReference[oaicite:6]{index=6}  \n",
    "- **Protocolo de grabaciÃ³n:**  \n",
    "  - PresentaciÃ³n visual de imÃ¡genes de movimientos en computadora.  \n",
    "  - Cada bloque de grabaciÃ³n duraba ~2-3 min, con perÃ­odos de reposo inicial (8 s) y cues (4-6 s) intercalados aleatoriamente. :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  12 Movimientos Decodificados  \n",
    "1. Reposo  \n",
    "2. Mano abierta  \n",
    "3. Cierre de mano (puÃ±o)  \n",
    "4. Pinza con Ã­ndice  \n",
    "5. FlexiÃ³n de muÃ±eca  \n",
    "6. ExtensiÃ³n de muÃ±eca  \n",
    "7. Pulgar arriba  \n",
    "*(Presumiblemente se incluyen mÃ¡s movimientos hasta completar 12, incluyendo distintos agarres y pronosupinaciÃ³n.)* :contentReference[oaicite:8]{index=8}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ“Š Resultados clave  \n",
    "- **PrecisiÃ³n general (12 movimientos + reposo):** 77.1 % Â± 5.6 % en participantes con stroke. :contentReference[oaicite:9]{index=9}  \n",
    "- **Pacientes con mano gravemente afectada (sÃ³lo 2 movimientos + reposo):** PrecisiÃ³n de 85.4 % Â± 6.4 %. :contentReference[oaicite:10]{index=10}  \n",
    "- **Escenarios en lÃ­nea (online) con dos participantes, comparando tres movimientos + reposo:** Alcanzaron 91.34 % Â± 1.53 %. :contentReference[oaicite:11]{index=11}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ Usabilidad (evaluaciÃ³n subjetiva)  \n",
    "Los participantes evaluaron varios aspectos del dispositivo en una escala de 1 a 5 (mÃ¡s alto = mejor):  \n",
    "- **FÃ¡cil de poner/Quitar (donning/doffing):** 3.60 Â± 0.28  \n",
    "- **Comodidad con uso prolongado (>1.5 h):** 4.57 Â± 0.20  \n",
    "- **Libertad de movimiento:** 4.07 Â± 0.32  \n",
    "- **Confianza de uso en actividades livianas en casa:** 4.07 Â± 0.22  \n",
    "- **SatisfacciÃ³n con el diseÃ±o visual:** 4.36 Â± 0.24  \n",
    "- **Favorabilidad general:** 4.79 Â± 0.15 :contentReference[oaicite:12]{index=12}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ RelaciÃ³n entre capacidad motora y decodificaciÃ³n  \n",
    "La precisiÃ³n del decodificador correlacionÃ³ positivamente con la habilidad motora observada. Movimientos con puntuaciones visibles mayores tuvieron mejor precisiÃ³n, aunque incluso sin movimiento visible (score = 0), el sistema logrÃ³ distinguir la intenciÃ³n de movimiento. :contentReference[oaicite:13]{index=13}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ Datos disponibles / Acceso  \n",
    "Los **datos del estudio no estÃ¡n disponibles pÃºblicamente**, pero pueden obtenerse **mediante solicitud razonable a los autores**. :contentReference[oaicite:14]{index=14}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ‘©â€ğŸ”¬ Autores / Afiliaciones  \n",
    "- **Autores principales:** Eric C. Meyers, David Gabrieli, Nick Tacca, Lauren Wengerd, Michael Darrow, Bryan R. Schlink, Ian Baumgart, David A. Friedenberg :contentReference[oaicite:15]{index=15}  \n",
    "- **Afiliaciones:** Battelle Memorial Institute (Divisiones de Medical Device Solutions y Health Analytics) :contentReference[oaicite:16]{index=16}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ›  Uso en Python (hipotÃ©tico)  \n",
    "Aunque los datos no son accesibles directamente, aquÃ­ tienes un ejemplo de cÃ³mo podrÃ­as estructurar un script si los tuvieras como arrays NumPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# SupÃ³n que X_stroke y y_labels contienen EMG y etiquetas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stroke, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(f\"PrecisiÃ³n decodificaciÃ³n: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba894d7",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£0ï¸âƒ£ [Improving Fast EMG Classification for Hand Gesture Recognition: A Comprehensive Analysis of Temporal, Spatial, and Algorithm Configurations for Healthy and Post-Stroke Subjects](https://www.preprints.org/manuscript/202505.0374/v1)  ğŸ“š\n",
    "\n",
    "ğŸ“Œ **Preprint publicaciÃ³n:** VersiÃ³n 1, enviada el 06 mayo 2025, publicada el 08 mayo 2025 (no revisada por pares) :contentReference[oaicite:0]{index=0}  \n",
    "ğŸ“Œ **Repositorio / Fuente:** Preprints.org (categorÃ­a IngenierÃ­a / BioingenierÃ­a) :contentReference[oaicite:1]{index=1}  \n",
    "ğŸ“Œ **Licencia / DOI:** DOI disponible: 10.20944/preprints202505.0374.v1 (licencia no especificada) :contentReference[oaicite:2]{index=2}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ” DescripciÃ³n general  \n",
    "Este estudio evalÃºa el rendimiento de la clasificaciÃ³n de seÃ±ales EMG en seis gestos manuales, tanto en individuos sanos como en pacientes post-stroke, explorando cÃ³mo el tiempo de adquisiciÃ³n (0,5â€“4 s) y el nÃºmero de canales (1 a 4) afectan la precisiÃ³n, robustez y generalizaciÃ³n. Se analizaron mÃºltiples mÃ©todos de extracciÃ³n de caracterÃ­sticas y modelos de aprendizaje automÃ¡tico. :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "- **Participantes:**  \n",
    "  - Individuos sanos  \n",
    "  - Pacientes post-stroke (para pruebas de generalizaciÃ³n cruzada) :contentReference[oaicite:4]{index=4}  \n",
    "- **Gestos clasificados:** seis gestos de mano y dedos, incluidos reposo, flexiÃ³n/extensiÃ³n de muÃ±eca, agarre, abducciÃ³n de dedos y supinaciÃ³n :contentReference[oaicite:5]{index=5}  \n",
    "\n",
    "---\n",
    "\n",
    "##  âš™ï¸ InstrumentaciÃ³n y protocolo  \n",
    "- **AdquisiciÃ³n en sujetos sanos:**  \n",
    "  - 40 participantes (edad 18â€“29), balance de gÃ©nero, 3 zurdos, 1 ambidiestro :contentReference[oaicite:6]{index=6}  \n",
    "  - Dispositivo BIOPAC MP36 con 4 canales: mÃºsculos extensor carpi ulnaris, flexor carpi ulnaris, extensor carpi radialis, flexor carpi radialis  \n",
    "  - Muestreo EMG a 2 kHz, filtrado banda 5â€“500 Hz mÃ¡s notch 50 Hz :contentReference[oaicite:7]{index=7}  \n",
    "- **AdquisiciÃ³n en pacientes post-stroke:**  \n",
    "  - Protocolo clÃ­nico Ã©tico y estandarizado  \n",
    "  - ElectrÃ³nica similar usando electrodes Ag/AgCl y filtro banda 5â€“500 Hz, tambiÃ©n a 2 kHz, sistema Human SpikerBox :contentReference[oaicite:8]{index=8}  \n",
    "  - Secuencias con preparaciÃ³n (~3 min), gesto (~4 s), reposo (~5 s), 50 repeticiones por gesto (~1 h por paciente) :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ Estructura de datos  \n",
    "No se publica directamente el dataset completo; sin embargo, se indica que los datos saludables estÃ¡n disponibles desde la fuente previa. Los datos de pacientes no se facilitan pÃºblicamente. Los hiperparÃ¡metros optados se incluyen como material suplementario :contentReference[oaicite:10]{index=10}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ MÃ©todos â€“ ExtracciÃ³n y clasificaciÃ³n  \n",
    "- **ExtracciÃ³n de caracterÃ­sticas:**  \n",
    "  - Power Spectral Density (PSD) (usando mÃ©todo de Welch, ventana 256 muestras, solapamiento 128, ventana Hanning)  \n",
    "  - Discrete Wavelet Transform (DWT) con wavelet Daubechies (db2), nivel de descomposiciÃ³n 3â€“4  \n",
    "  - ReducciÃ³n dimensional por PCA o SVD (98 % varianza retenida) :contentReference[oaicite:11]{index=11}  \n",
    "- **Modelos evaluados:**  \n",
    "  - Random Forest (RF), Support Vector Machine (SVM), Neural Network (MLP)  \n",
    "  - ValidaciÃ³n con split de 80 % entrenamiento / 20 % prueba, 5-fold cross-validation, optimizaciÃ³n por grid search, seed = 42 :contentReference[oaicite:12]{index=12}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ DiseÃ±o experimental  \n",
    "- Se probaron 315 configuraciones combinando:  \n",
    "  - 9 ventanas temporales desde 0,5 s hasta 4 s  \n",
    "  - 7 configuraciones de canales (1, 2 o 4 canales combinados)  \n",
    "  - 5 mÃ©todos de extracciÃ³n (PSD, PSD+PCA, PSD+SVD, DWT+PCA, DWT+SVD)  \n",
    "  - 3 algoritmos de clasificaciÃ³n (RF, SVM, NN) :contentReference[oaicite:13]{index=13}  \n",
    "- AdemÃ¡s: pruebas de robustez con mÃºltiples particiones, anÃ¡lisis de generalizaciÃ³n intra- e inter-paciente :contentReference[oaicite:14]{index=14}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ Resultados clave  \n",
    "- **ExtracciÃ³n:** PSD + PCA o PSD + SVD rindiÃ³ mejor que DWT. RF mostrÃ³ mayor robustez con DWT. :contentReference[oaicite:15]{index=15}  \n",
    "- **Ventana temporal:** La precisiÃ³n mejora significativamente al aumentar la ventana de 0,5 s a 2 s, pero se estabiliza mÃ¡s allÃ¡ de 2 s. :contentReference[oaicite:16]{index=16}  \n",
    "- **NÃºmero de canales:** Aumentar de 1 a 2 canales tiene mayor impacto que de 2 a 4. Combinado con ventana â‰¥ 2 s, mejora sustancial. :contentReference[oaicite:17]{index=17}  \n",
    "- **Robustez:** PSD+PCA presentaba coeficiente de variaciÃ³n bajo (<5 %); DWT+SVD mostrÃ³ variabilidad mayor (6â€“10 %). RF fue mÃ¡s consistente mientras que NN fue menos robusto. :contentReference[oaicite:18]{index=18}  \n",
    "- **GeneralizaciÃ³n en pacientes:**  \n",
    "  - Intra-paciente: hasta ~80 % con 30 muestras por clase; depende del modelo (RF, SVM, NN diferencias sutiles) :contentReference[oaicite:19]{index=19}  \n",
    "  - Inter-paciente: rendimiento se desploma a ~35â€“40 % â†’ necesidad de adaptaciÃ³n o entrenamiento por usuario :contentReference[oaicite:20]{index=20}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ Conclusiones y recomendaciones  \n",
    "- El enfoque ideal para clasificaciÃ³n rÃ¡pida y precisa de gestos EMG usa:  \n",
    "  - 2 segundos de ventana temporal  \n",
    "  - 2 canales de EMG  \n",
    "  - ExtracciÃ³n PSD + PCA  \n",
    "  - RF como algoritmo robusto :contentReference[oaicite:21]{index=21}  \n",
    "- Para aplicaciones real-time, esta configuraciÃ³n reduce latencia y complejidad sin sacrificar precisiÃ³n.  \n",
    "- Sistemas personalizados (entrenados por paciente) son mÃ¡s viables que modelos genÃ©ricos. Futuras mejoras incluyen deep learning y validaciÃ³n en escenarios reales. :contentReference[oaicite:22]{index=22}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ Autores  \n",
    "- Camila Montecinos (primera autora), Jessica Espinoza, MÃ³nica Zamora Zapata, Viviana Meruane, RubÃ©n FernÃ¡ndez :contentReference[oaicite:23]{index=23}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ›  Uso en Python (hipotÃ©tico bÃ¡sico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9cf229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# X_emg: datos EMG reshapeados (n_samples, n_features), y_labels: etiquetas\n",
    "pca = PCA(n_components=0.98)\n",
    "X_reduced = pca.fit_transform(X_emg)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "param_grid = {'n_estimators': [100, 150, 200], 'criterion': ['gini','entropy']}\n",
    "grid = GridSearchCV(rf, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejor RF:\", grid.best_params_, \"Accuracy:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea709843",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£1ï¸âƒ£ [U-Limb: A multi-modal, multi-center database on arm motion control in healthy and post-stroke conditions](https://pubmed.ncbi.nlm.nih.gov/34143875/)  ğŸ“š\n",
    "\n",
    "ğŸ“Œ **PublicaciÃ³n:** 18 de junio de 2021 â€“ *GigaScience* (Vol. 10, NÃºm. 6) :contentReference[oaicite:0]{index=0}  \n",
    "ğŸ“Œ **Repositorio / Fuente:** GigaScience, Oxford University Press (acceso abierto) :contentReference[oaicite:1]{index=1}  \n",
    "ğŸ“Œ **Licencia / DOI:** DOI: 10.1093/gigascience/giab043 :contentReference[oaicite:2]{index=2}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ” DescripciÃ³n general  \n",
    "U-Limb es una extensa base de datos multimodal y multicÃ©ntrica diseÃ±ada para profundizar en la comprensiÃ³n del control motor del brazo:\n",
    "\n",
    "- Participantes: **91 sujetos sanos** y **65 pacientes post-stroke** :contentReference[oaicite:3]{index=3}  \n",
    "- **Tres niveles de datos recopilados**:\n",
    "  1. Actividades cotidianas del miembro superior con registros de **kinemÃ¡tica y seÃ±ales fisiolÃ³gicas** (EMG, EEG, ECG)  \n",
    "  2. Comportamiento cinÃ©tico-tÃ¡ctil durante tareas de manipulaciÃ³n precisas con dispositivo hÃ¡ptico  \n",
    "  3. Actividad cerebral durante el control de la mano, captada por **fMRI** :contentReference[oaicite:4]{index=4}  \n",
    "\n",
    "---\n",
    "\n",
    "##  âš™ï¸ InstrumentaciÃ³n y protocolo  \n",
    "- **Sensores EMG**: colocados siguiendo directrices SENIAM  \n",
    "- **KinemÃ¡tica**: marcadores activos para captura de movimiento (motion capture) con precisiÃ³n anatÃ³mica  \n",
    "- **Tareas incluidas**:  \n",
    "  - Virtual Peg Insertion Test â€” prueba funcional que combina cinemÃ¡tica, fuerza y realidad virtual :contentReference[oaicite:5]{index=5}  \n",
    "  - Estudios en mÃºltiples centros (como Pisa, MHH, TUM, UP), con alineaciÃ³n rigurosa de procedimientos :contentReference[oaicite:6]{index=6}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ Estructura del dataset  \n",
    "Aunque el artÃ­culo describe ampliamente los datos recogidos, no detalla el formato exacto (archivos, variables o estructura interna). No obstante, estÃ¡ claro que incluye:\n",
    "\n",
    "- Registros EMG, EEG, ECG (seÃ±ales fisiolÃ³gicas)  \n",
    "- Datos de movimiento (cinemÃ¡tica)  \n",
    "- Registros de fuerza kinÃ©sica durante manipulaciÃ³n  \n",
    "- Datos de actividad cerebral (fMRI) :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ CaracterÃ­sticas del dataset  \n",
    "- **Tipo:** Datos multimodales (EMG, EEG, ECG, cinemÃ¡tica, fuerzas, neuroimagen)  \n",
    "- **Ãrea temÃ¡tica:** Neurociencia del movimiento, rehabilitaciÃ³n, robÃ³tica asistiva, interacciÃ³n humano-mÃ¡quina  \n",
    "- **Tareas potenciales:** AnÃ¡lisis del control motor, rehabilitaciÃ³n personalizada, diseÃ±o de prÃ³tesis e interfaces adaptativas  \n",
    "\n",
    "---\n",
    "\n",
    "##  ï¸ Autores y afiliaciones  \n",
    "- **Autores principales:** Giuseppe Averta, Federica Barontini, Vincenzo Catrambone, Sami Haddadin, Giacomo Handjaras, entre otros :contentReference[oaicite:8]{index=8}  \n",
    "- **Centros participantes:**  \n",
    "  - Universidad de Pisa & Istituto Italiano di Tecnologia (Italia)  \n",
    "  - Technical University Munich (Alemania)  \n",
    "  - Hannover Medical School (Alemania)  \n",
    "  - University of Zurich (Suiza), entre otros :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ğŸ›  Uso en Python (ejemplo genÃ©rico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo ilustrativo: carga estructurada de datos multimodales\n",
    "import numpy as np\n",
    "\n",
    "# SimulaciÃ³n de carga (ajustar segÃºn formato real, por ejemplo .mat, .csv, .npy, etc.)\n",
    "emg = np.load('u_limb_emg_healthy.npy')    # (n_samples, n_channels)\n",
    "eeg = np.load('u_limb_eeg_healthy.npy')    # (n_samples, n_channels)\n",
    "kin = np.load('u_limb_kinematics_healthy.npy')  # (n_samples, ...)\n",
    "\n",
    "print(\"EMG shape:\", emg.shape)\n",
    "print(\"EEG shape:\", eeg.shape)\n",
    "print(\"Kinematics shape:\", kin.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
