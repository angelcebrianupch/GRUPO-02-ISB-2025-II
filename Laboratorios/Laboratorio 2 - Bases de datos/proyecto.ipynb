{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3214db25",
   "metadata": {},
   "source": [
    "# ✨ Bases de Datos EMG - Grupo 02\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Tema del Proyecto\n",
    "**Hand Gesture Recognition with EMG Signals**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Problemática  \n",
    "Los pacientes que han sufrido un **ictus** enfrentan serias dificultades para recuperar el control motor fino de sus manos.  \n",
    "Una herramienta que permita **monitorear la recuperación motora** mediante el reconocimiento de gestos con señales EMG sería de gran utilidad.  \n",
    "\n",
    "Actualmente, los sistemas disponibles son:  \n",
    "- 💰 **Costosos**  \n",
    "- ⚙️ **Complejos de implementar**  \n",
    "- 🚫 **Poco accesibles en contextos clínicos comunes**  \n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Propuesta de Solución  \n",
    "✅ Recopilar señales **EMG** desde los músculos del antebrazo.  \n",
    "✅ Entrenar un **clasificador supervisado** (*KNN, SVM, etc.*).  \n",
    "✅ Reconocer entre **3 y 5 gestos sencillos de la mano** para su aplicación en rehabilitación.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Objetivos de esta Sección (Bases de Datos)\n",
    "\n",
    "📂 **Explorar** repositorios públicos con señales EMG.  \n",
    "📊 **Comparar** sus características principales (frecuencia de muestreo, número de sujetos, cantidad de gestos).  \n",
    "🔎 **Seleccionar** el dataset más adecuado para entrenar y validar el clasificador.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03625bad",
   "metadata": {},
   "source": [
    "# 📑 Base de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7bbc7",
   "metadata": {},
   "source": [
    "# 0️⃣1️⃣ [Gesture Recognition and Biometrics ElectroMyogram (GRABMyo)](https://physionet.org/content/grabmyo/1.1.0/ ) 📚\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Información General\n",
    "- **Nombre completo:** *Gesture Recognition and Biometrics ElectroMyogram (GRABMyo)*  \n",
    "- **Autores:** Ning Jiang, Ashirbad Pradhan, Jiayuan He  \n",
    "- **Publicado en:** PhysioNet (versión 1.1.0, junio 7, 2024)  \n",
    "- **DOI:** [10.13026/89dm-f662](https://doi.org/10.13026/89dm-f662)  \n",
    "- **Sujetos:** 43 participantes (23 hombres, 20 mujeres), edad: 24–35 años  \n",
    "- **Sesiones:** 3 días distintos → *129 grabaciones en total*  \n",
    "- **Gestos registrados:** 16 gestos de mano y dedos + reposo  \n",
    "- **Frecuencia de muestreo:** 2048 Hz  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://physionet.org/files/grabmyo/1.1.0/GestureList.JPG?download\" alt=\"gramb_refernce\" height=\"700\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Propósito del Dataset\n",
    "El dataset se diseñó para aplicaciones en:  \n",
    "1. 🔐 **Biometría EMG**: identificación y autenticación personal.  \n",
    "2. ✋ **Reconocimiento de gestos**: rehabilitación, prótesis, control de interfaces y entornos virtuales.  \n",
    "3. 🧪 **Robustez multisesión**: evaluar variaciones entre días (electrodos, piel, fatiga, etc.).  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Diseño Experimental\n",
    "- **Electrodos:** 28 en total (16 antebrazo + 12 muñeca en configuración bipolar).  \n",
    "- **Protocolo:**  \n",
    "  - 17 gestos (16 movimientos + reposo).  \n",
    "  - Cada gesto repetido **7 veces**.  \n",
    "  - Duración: 5 s por gesto + 10 s de descanso.  \n",
    "- **Configuración de sesiones:**  \n",
    "  - `Session1/Session2/Session3`  \n",
    "  - Cada una con 43 subcarpetas (un sujeto por carpeta).  \n",
    "  - Archivos de salida: `.dat` y `.hea`.  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ieee-dataport.org/sites/default/files/styles/home/public/Appendix1_2.jpg?itok=Z4M-sNmQ\" alt=\"gramb_refernce2\" height=\"300\">\n",
    "</p>\n",
    "---\n",
    "\n",
    "## 📊 Características Técnicas\n",
    "- **Canales activos:**  \n",
    "  - F1–F16 → antebrazo (2 anillos × 8 electrodos).  \n",
    "  - W1–W12 → muñeca (2 anillos × 6 electrodos).  \n",
    "  - U1–U4 → canales no usados (espaciadores).  \n",
    "- **Tamaño de cada registro:** `10240 x 32` (5 s × 2048 Hz × 32 canales).  \n",
    "- **Archivos extra:**  \n",
    "  - `grabmyo_convert_wfdb_to_mat.py` → conversión a MATLAB.  \n",
    "  - `grabmyo_feature_extraction.m` → extracción de características.  \n",
    "  - `grabmyo_visualize.py` → visualización de datos.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Posibles Usos\n",
    "- 🛡️ **Autenticación biométrica** → EMG como contraseña o firma muscular.  \n",
    "- 🆔 **Identificación** → reconocer al usuario entre N sujetos.  \n",
    "- 🦾 **Reconocimiento de gestos** → aplicaciones en rehabilitación, prótesis y control de interfaces.  \n",
    "- 🔄 **Adaptación a electrodos desplazados** → técnicas de transferencia y robustez.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a3958",
   "metadata": {},
   "source": [
    "# 0️⃣2️⃣ [sEMG for Basic Hand Movements – UCI Repository](https://archive.ics.uci.edu/dataset/313/semg+for+basic+hand+movements ) 📚\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Información General\n",
    "- **Nombre completo:** *sEMG for Basic Hand movements*  \n",
    "- **Autores:** Christos Sapsanis, Anthony Tzes, G. Georgoulas  \n",
    "- **Publicado en:** UCI Machine Learning Repository (2014)  \n",
    "- **DOI:** [10.24432/C5TK53](https://doi.org/10.24432/C5TK53)  \n",
    "- **Sujetos:**  \n",
    "  - **Database 1:** 5 sujetos (2 hombres, 3 mujeres, 20–22 años).  \n",
    "  - **Database 2:** 1 sujeto (hombre, 22 años) durante 3 días consecutivos.  \n",
    "- **Movimientos:** 6 tipos de agarre manual + reposo.  \n",
    "- **Frecuencia de muestreo:** 500 Hz.  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41597-022-01836-y/MediaObjects/41597_2022_1836_Fig1_HTML.png\" alt=\"semgrefernce2\" height=\"400\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Propósito del Dataset\n",
    "El dataset fue diseñado para:  \n",
    "1. 🧠 **Clasificación de movimientos de la mano** a partir de EMG superficial.  \n",
    "2. 💪 **Análisis de activación muscular** en movimientos funcionales.  \n",
    "3. 🦾 **Aplicaciones biomédicas** en rehabilitación, prótesis e interfaces hombre-máquina.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Diseño Experimental\n",
    "- **Electrodos utilizados:**  \n",
    "  - 2 canales → *Flexor Carpi Ulnaris* y *Extensor Carpi Radialis (longus y brevis)*.  \n",
    "  - Referencia en el antebrazo, fijados con bandas elásticas.  \n",
    "- **Sistema de adquisición:**  \n",
    "  - Delsys Bagnoli™ 2-channel EMG System.  \n",
    "  - NI USB-009 para conversión A/D.  \n",
    "- **Filtrado de la señal:**  \n",
    "  - **Band-pass Butterworth:** 15–500 Hz.  \n",
    "  - **Notch:** 50 Hz para eliminar ruido de línea.  \n",
    "- **Protocolo:**  \n",
    "  - Cada gesto repetido múltiples veces (30 o 100, según base).  \n",
    "  - Duración de las pruebas: 5–6 s.  \n",
    "  - Los sujetos ajustaban fuerza y velocidad libremente.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🖐️ Movimientos Registrados\n",
    "Los 6 gestos corresponden a agarres cotidianos:  \n",
    "1. **Spherical** → agarrar objetos redondos.  \n",
    "2. **Tip** → sujetar objetos pequeños.  \n",
    "3. **Palmar** → agarrar con palma abierta.  \n",
    "4. **Lateral** → sujetar objetos planos/finos.  \n",
    "5. **Cylindrical** → sostener cilindros.  \n",
    "6. **Hook** → cargar objetos pesados en forma de gancho.  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.researchgate.net/publication/357759649/figure/fig1/AS:11431281122325537@1677270339787/Gestures-performed-in-sEMG-for-Basic-Hand-movements-Data-Set-3.jpg\" alt=\"semgrefernce3\" height=\"300\">\n",
    "</p>\n",
    "---\n",
    "\n",
    "## 📊 Características del Dataset\n",
    "- **Database 1 (5 sujetos):**  \n",
    "  - 6 gestos × 30 repeticiones × 6 s.  \n",
    "  - Archivos `.mat` con **12 matrices por sujeto** (2 canales × 6 gestos).  \n",
    "  - Cada matriz: **30 filas × 3000 columnas** (señal en voltaje).  \n",
    "\n",
    "- **Database 2 (1 sujeto, 3 días):**  \n",
    "  - 6 gestos × 100 repeticiones × 5 s.  \n",
    "  - Archivos `.mat` por día.  \n",
    "  - Cada matriz: **100 filas × 2500 columnas**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Posibles Usos\n",
    "- 📈 **Clasificación multiclase** → modelos ML para reconocer gestos.  \n",
    "- 🦾 **Control de prótesis** y dispositivos de asistencia.  \n",
    "- 🏋️ **Estudio de variabilidad** → entre sujetos y entre días.  \n",
    "- 🛡️ **Interfaces biomédicas** → rehabilitación y neuroingeniería.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d726b",
   "metadata": {},
   "source": [
    "# 0️⃣3️⃣ [Dataset: EMG Data for Gestures](https://archive.ics.uci.edu/dataset/481/emg+data+for+gestures ) 📚\n",
    "\n",
    "📌 **Donación:** 06/01/2019  \n",
    "📌 **Repositorio:** UCI Machine Learning Repository  \n",
    "📌 **DOI:** [10.24432/C5ZP5C](https://doi.org/10.24432/C5ZP5C)  \n",
    "📌 **Licencia:** Creative Commons Attribution 4.0 International (CC BY 4.0)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Descripción general  \n",
    "Este dataset contiene señales **EMG crudas** registradas mediante un brazalete **Myo Thalmic**, el cual cuenta con **8 sensores** dispuestos alrededor del antebrazo.  \n",
    "\n",
    "- **Sujetos:** 36 voluntarios  \n",
    "- **Gestos estáticos registrados:** 6–7 tipos  \n",
    "- **Duración:** Cada gesto fue sostenido **3 segundos**, con **3 segundos de pausa** entre gestos.  \n",
    "- **Número de instancias:** entre **40,000–50,000 registros por archivo** (garantizados al menos 30,000).  \n",
    "- **Tareas posibles:** clasificación de gestos, procesamiento de señales biomédicas, biometría.  \n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Instrumentación y protocolo  \n",
    "- **Dispositivo:** Myo Thalmic Bracelet  \n",
    "- **Sensores:** 8 canales EMG distribuidos en el antebrazo  \n",
    "- **Conexión:** Bluetooth a PC  \n",
    "- **Datos adquiridos:** señales EMG crudas (sin filtrado previo)  \n",
    "\n",
    "Cada sujeto realizó **2 series de gestos**:  \n",
    "1. Mano en reposo  \n",
    "2. Mano cerrada en puño  \n",
    "3. Flexión de muñeca  \n",
    "4. Extensión de muñeca  \n",
    "5. Desviación radial  \n",
    "6. Desviación cubital  \n",
    "7. Palma extendida (*no todos los sujetos la realizaron*)  \n",
    "\n",
    "---\n",
    "\n",
    "## 📂 Estructura de los archivos  \n",
    "Cada archivo de datos contiene **10 columnas**:  \n",
    "\n",
    "1. **Tiempo (ms)**  \n",
    "2–9. **Canales EMG** (8 sensores del brazalete)  \n",
    "10. **Etiqueta (gesto):**  \n",
    "   - `0` → sin marcar  \n",
    "   - `1` → mano en reposo  \n",
    "   - `2` → puño cerrado  \n",
    "   - `3` → flexión de muñeca  \n",
    "   - `4` → extensión de muñeca  \n",
    "   - `5` → desviación radial  \n",
    "   - `6` → desviación cubital  \n",
    "   - `7` → palma extendida  \n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Características del dataset  \n",
    "- **Tipo:** Series temporales  \n",
    "- **Área temática:** Salud y medicina  \n",
    "- **Tarea asociada:** Clasificación  \n",
    "- **Formato de datos:** Texto plano (.txt)  \n",
    "- **Valores faltantes:** No presenta  \n",
    "\n",
    "---\n",
    "\n",
    "## 📂 Archivos disponibles (ejemplos)  \n",
    "- `1_raw_data_13-11_18.03.16.txt` (4.4 MB)  \n",
    "- `1_raw_data_10-51_07.04.16.txt` (4.5 MB)  \n",
    "- `2_raw_data_13-29_21.03.16.txt` (4.6 MB)  \n",
    "*(73 archivos en total, organizados por sujeto y sesión)*  \n",
    "\n",
    "---\n",
    "\n",
    "## 👩‍🔬 Autores  \n",
    "- N. Krilova  \n",
    "- I. Kastalskiy  \n",
    "- V. Kazantsev  \n",
    "- V.A. Makarov  \n",
    "- S. Lobov  \n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Uso en Python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el dataset\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Cargar datos\n",
    "emg_data_for_gestures = fetch_ucirepo(id=481)\n",
    "\n",
    "# Señales y etiquetas\n",
    "X = emg_data_for_gestures.data.features\n",
    "y = emg_data_for_gestures.data.targets\n",
    "\n",
    "# Metadatos\n",
    "print(emg_data_for_gestures.metadata)\n",
    "\n",
    "# Variables\n",
    "print(emg_data_for_gestures.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef750a85",
   "metadata": {},
   "source": [
    "# 0️⃣4️⃣[DB1: Ninapro EMG & Kinematic Data for Hand Movements](https://ninapro.hevs.ch/instructions/DB1.html) 📚\n",
    "📌 **Repositorio:** Ninapro (Swiss National Science Foundation)  \n",
    "---\n",
    "\n",
    "##  🔎 Descripción general  \n",
    "Este dataset incluye señales **sEMG** y datos **cinemáticos** registrados de **27 sujetos intactos**, repitiendo **52 movimientos de mano** más posición de reposo. :contentReference[oaicite:0]{index=0}  \n",
    "- **Sujetos:** 27 personas (23 diestros derechos y 4 zurdos), edades entre aprox. 22 y 40 años :contentReference[oaicite:1]{index=1}  \n",
    "- **Movimientos:** 52 gestos distintos de mano, además de reposo, organizados en tres ejercicios:  \n",
    "  1. Movimientos básicos de los dedos  \n",
    "  2. Configuraciones isométricas, isotónicas del puño y movimientos de muñeca  \n",
    "  3. Agarres y movimientos funcionales :contentReference[oaicite:2]{index=2}  \n",
    "- **Repeticiones:** 10 repeticiones por gesto :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ⚙️ Instrumentación y protocolo  \n",
    "- **Dispositivos:**  \n",
    "  - 10 electrodos sEMG “Otto Bock MyoBock 13E200” (8 colocados alrededor del antebrazo, más dos sobre flexor y extensor digitorum superficialis) :contentReference[oaicite:4]{index=4}  \n",
    "  - CyberGlove II para captura cinemática (22 sensores de ángulo sin calibración) :contentReference[oaicite:5]{index=5}  \n",
    "- **Protocolo de adquisición:** Los sujetos repitieron los movimientos guiados mediante videos en laptop, siguiendo las tres fases descritas arriba :contentReference[oaicite:6]{index=6}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🗂 Estructura de los archivos  \n",
    "Cada sujeto y ejercicio cuenta con un archivo **MATLAB (.mat)** que incluye variables sincronizadas:  \n",
    "- `Emg` (10 columnas): señales sEMG (8 alrededor + 2 del flexor/extensor)  \n",
    "- `Glove` (22 columnas): señales sin calibrar del CyberGlove  \n",
    "- `Stimulus`: etiqueta del movimiento mostrado  \n",
    "- `Restimulus`: etiqueta refinada post-adquisición  \n",
    "- `Repetition`: número de repetición del estímulo  \n",
    "- `Rerepetition`: repetición del restimulus :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  📊 Características del dataset  \n",
    "- **Tipo:** Series temporales sin procesar  \n",
    "- **Área:** Control de prótesis, biomecánica, biometría  \n",
    "- **Tareas:** Clasificación de gestos, modelado de movimiento  \n",
    "- **Formato:** Archivos `.mat` (MATLAB), posible conversión a otros formatos  \n",
    "- **Valores faltantes:** No se reportan :contentReference[oaicite:8]{index=8}  \n",
    "\n",
    "---\n",
    "\n",
    "##  📂 Archivos disponibles  \n",
    "- Para cada sujeto (1 a 27), un archivo `.zip` (p. ej., `s1.zip`, `s2.zip`, ..., `s27.zip`) con los datos correspondientes :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  👩‍🔬 Autores / Referencias  \n",
    "- **Publicaciones asociadas:**  \n",
    "  - Atzori et al., *Characterization of a Benchmark Database for Myoelectric Movement Classification*, IEEE T-NSRE, 2014  \n",
    "  - *Electromyography data for non-invasive naturally-controlled robotic hand prostheses*, Scientific Data, 2014  \n",
    "  - *Building the NINAPRO Database: A Resource for the Biorobotics Community* :contentReference[oaicite:10]{index=10}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🛠 Uso en Python (ejemplo básico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058644b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Cargar archivo .mat de un sujeto / ejercicio\n",
    "data = scipy.io.loadmat('s1_ex1.mat')\n",
    "\n",
    "emg = data['Emg']           # (N_samples, 10)\n",
    "glove = data['Glove']       # (N_samples, 22)\n",
    "stim = data['Stimulus'].flatten()\n",
    "restim = data['Restimulus'].flatten()\n",
    "rep = data['Repetition'].flatten()\n",
    "\n",
    "print(\"EMG shape:\", emg.shape)\n",
    "print(\"First 5 Stimuli:\", stim[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e54e5e",
   "metadata": {},
   "source": [
    "# 0️⃣5️⃣[EMG data of stroke and healthy subjects when performing hand-to-nose movement](https://figshare.com/articles/dataset/) 📚\n",
    "\n",
    "📌 **Donación / Publicación:** Aproximadamente hace 3.6 años (~2021) :contentReference[oaicite:0]{index=0}  \n",
    "📌 **Repositorio:** Figshare (Kunkun Zhao) :contentReference[oaicite:1]{index=1}  \n",
    "📌 **Licencia:** No especificada en la fuente pública  \n",
    "📌 **DOI:** No se menciona un DOI específico en la descripción :contentReference[oaicite:2]{index=2}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🔎 Descripción general  \n",
    "Este dataset contiene señales **EMG en bruto** registradas durante el movimiento “mano a la nariz”, tanto de **pacientes con accidente cerebrovascular (stroke)** como de **sujetos sanos**. Incluye también puntuaciones funcionales Fugl-Meyer (FM) de 20 pacientes con stroke.  \n",
    "- **Archivos incluidos** (3 archivos):  \n",
    "  - `FM.mat`: puntuaciones funcionales de miembros superiores (Fugl-Meyer) para 20 pacientes con stroke.  \n",
    "  - `rawEMGData_H.mat`: señales EMG de sujetos sanos.  \n",
    "  - `rawEMGData_P.mat`: señales EMG de pacientes con stroke. :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ⚙️ Instrumentación y protocolo  \n",
    "- **Dispositivos / configuración:** No se especifican los detalles del equipo utilizado (tipo de sensores, ubicación de electrodos, frecuencia de muestreo, etc.) en la fuente consultada.  \n",
    "- **Protocolo:** Se registra el movimiento mano-a-nariz en participantes sanos y pacientes post-stroke, aunque el protocolo exacto (repeticiones, duración, condiciones estándar) no se detalla en la descripción pública.  \n",
    "\n",
    "---\n",
    "\n",
    "##  🗂 Estructura de los archivos  \n",
    "Los archivos `.mat` incluidos contienen lo siguiente:  \n",
    "- `FM.mat`: puntuaciones Fugl-Meyer para la función motora de 20 pacientes con stroke.  \n",
    "- `rawEMGData_H.mat` y `rawEMGData_P.mat`: señales EMG en bruto de los sujetos sanos (`H`) y de los pacientes con stroke (`P`). :contentReference[oaicite:4]{index=4}  \n",
    "\n",
    "No se detalla la estructura interna (número de canales, etiquetas, formato temporal), ya que la fuente no proporciona información específica al respecto.\n",
    "\n",
    "---\n",
    "\n",
    "##  📊 Características del dataset  \n",
    "- **Tipo:** Series temporales (EMG crudo)  \n",
    "- **Área temática:** Rehabilitación, neurología, biomecánica  \n",
    "- **Tarea asociada:** Análisis de señales EMG y posible clasificación o correlación con función motora (Fugl-Meyer)  \n",
    "- **Formato:** Archivos `.mat` (MATLAB)  \n",
    "- **Valores faltantes:** No se especifica si hay valores faltantes o no :contentReference[oaicite:5]{index=5}  \n",
    "\n",
    "---\n",
    "\n",
    "##  📂 Archivos disponibles  \n",
    "- `FM.mat` – puntuaciones Fugl-Meyer para 20 pacientes con accidente cerebrovascular  \n",
    "- `rawEMGData_H.mat` – datos EMG de sujetos sanos  \n",
    "- `rawEMGData_P.mat` – datos EMG de pacientes con stroke :contentReference[oaicite:6]{index=6}  \n",
    "\n",
    "---\n",
    "\n",
    "##  👩‍🔬 Autores / Referencias  \n",
    "- **Autor(es):** Kunkun Zhao y colaboradores (según figshare) :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🛠 Uso en Python (ejemplo básico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb392e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos EMG de sujetos sanos\n",
    "data_h = scipy.io.loadmat('rawEMGData_H.mat')\n",
    "# Cargar datos EMG de pacientes con stroke\n",
    "data_p = scipy.io.loadmat('rawEMGData_P.mat')\n",
    "# Cargar puntuaciones Fugl-Meyer\n",
    "fm = scipy.io.loadmat('FM.mat')\n",
    "\n",
    "# Ejemplo de exploración básica\n",
    "print(\"Claves disponibles en rawEMGData_H:\", data_h.keys())\n",
    "print(\"Claves disponibles en rawEMGData_P:\", data_p.keys())\n",
    "print(\"Claves disponibles en FM.mat:\", fm.keys())\n",
    "\n",
    "# Si hay una variable clave 'emg', podríamos hacer:\n",
    "# emg_h = data_h['emg']\n",
    "# print(\"Shape EMG sujetos sanos:\", emg_h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dd9be",
   "metadata": {},
   "source": [
    "# 0️⃣6️⃣ [EMG datasets for hand-reaching movements for multiple directions of healthy and post-stroke individuals](https://data.mendeley.com/datasets/f4hh43nd78/1)  📚 \n",
    "\n",
    "📌 **Donación / Publicación:** 5 de junio de 2018 (Versión 1)  \n",
    "📌 **Repositorio:** Mendeley Data (Universidad de Haifa)  \n",
    "📌 **Licencia:** Creative Commons Attribution 4.0 International (CC BY 4.0) :contentReference[oaicite:0]{index=0}  \n",
    "📌 **DOI:** 10.17632/f4hh43nd78.1 :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "---\n",
    "\n",
    "##  🔎 Descripción general  \n",
    "Este dataset contiene señales **EMG multi-canal** registradas durante movimientos de alcance manual en **individuos sanos y post-stroke**. Los participantes realizaron movimientos de alcance hacia **9 direcciones objetivo** bajo monitoreo de sensores EMG. :contentReference[oaicite:2]{index=2}  \n",
    "- **Participantes:** Carpeta “Healthy” con 12 sujetos sanos y carpeta “Post-stroke” con 13 sujetos. :contentReference[oaicite:3]{index=3}  \n",
    "- **Movimientos:** Alcance manual hacia 9 direcciones consistentes entre todos los sujetos. :contentReference[oaicite:4]{index=4}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ⚙️ Instrumentación y protocolo  \n",
    "- **Sensores:** 8 electrodos de superficie EMG colocados en 8 músculos del hombro y brazo. :contentReference[oaicite:5]{index=5}  \n",
    "- **Protocolo:**  \n",
    "  1. Mediciones de **Máxima Contracción Voluntaria (MVC)** de los músculos (8 archivos por sujeto).  \n",
    "  2. Archivos denominados `Target_XXX` para cada dirección de alcance (9 archivos para sujetos sanos, 6 para algunos post-stroke). :contentReference[oaicite:6]{index=6}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🗂 Estructura de los archivos  \n",
    "En cada carpeta individual (por sujeto) se encuentran:  \n",
    "- 8 archivos MVC (Máxima Contracción Voluntaria).  \n",
    "- Archivos `Target_XXX`, cada uno con señales de los 8 músculos durante el movimiento hacia una dirección específica. :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  📊 Características del dataset  \n",
    "- **Tipo:** Series temporales EMG sin procesar.  \n",
    "- **Área temática:** Rehabilitación, accidente cerebrovascular (stroke), control motor. :contentReference[oaicite:8]{index=8}  \n",
    "- **Tareas asociadas:** Comparación entre grupos (sanos vs post-stroke), análisis de fuerza muscular, clasificación de direcciones de movimiento.  \n",
    "- **Formato:** Archivos (tipo no especificado en la descripción, presumiblemente binarios legibles o formatos de texto estructurado).  \n",
    "- **Valores faltantes:** No se indica su existencia en la fuente consultada.  \n",
    "\n",
    "---\n",
    "\n",
    "##  📂 Archivos disponibles  \n",
    "- **Para cada sujeto sano (12):**  \n",
    "  - 8 archivos MVC  \n",
    "  - 9 archivos `Target_XXX` (direcciones de alcance)  \n",
    "- **Para cada sujeto post-stroke (13):**  \n",
    "  - 8 archivos MVC  \n",
    "  - 6 archivos `Target_XXX`, según disponibilidad de direcciones. :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  👩‍🔬 Autores / Referencias  \n",
    "- **Contribuyente principal:** Sharon Israely (Universidad de Haifa) :contentReference[oaicite:10]{index=10}\n",
    "\n",
    "---\n",
    "\n",
    "##  🛠 Uso en Python (ejemplo básico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c270ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ejemplo: listar archivos de un sujeto\n",
    "subject_folder = 'Healthy/subject01'  # ajustar ruta local\n",
    "files = os.listdir(subject_folder)\n",
    "print(\"Archivos disponibles:\", files)\n",
    "\n",
    "# Cargar un archivo (si es formato texto o numpy)\n",
    "# Ajusta según formato real, a modo de ejemplo:\n",
    "# emg_data = np.loadtxt(os.path.join(subject_folder, files[0]))\n",
    "# print(\"Shape de emg_data:\", emg_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a9ea1",
   "metadata": {},
   "source": [
    "# 0️⃣7️⃣ [Post-stroke hand gesture recognition via one-shot transfer learning using prototypical networks](https://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-024-01398-7)  📚\n",
    "\n",
    "📌 **Publicación:** 12 de junio de 2024 :contentReference[oaicite:0]{index=0}  \n",
    "📌 **Repositorio / Fuente:** Journal of NeuroEngineering and Rehabilitation (Open Access, BMC / Springer Nature) :contentReference[oaicite:1]{index=1}  \n",
    "📌 **Licencia:** Creative Commons Attribution 4.0 International (CC BY 4.0) – datos abiertos incluidos :contentReference[oaicite:2]{index=2}  \n",
    "📌 **DOI:** 10.1186/s12984-024-01398-7 :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🔎 Descripción general  \n",
    "Este estudio aborda el reconocimiento de gestos manuales en pacientes post-stroke mediante aprendizaje de transferencia de un solo ejemplo (one-shot transfer learning), utilizando **prototypical networks** —un tipo de red neuronal entrenada para distinguir gestos con muy pocos ejemplos.\n",
    "\n",
    "- **Sujetos participantes:** 20 personas con antecedente de accidente cerebrovascular (stroke), reclutados en el Hospital Huashan, Shanghái :contentReference[oaicite:4]{index=4}  \n",
    "- **Gestos evaluados:** 7 movimientos relevantes para actividades diarias:\n",
    "  1. Flexión masiva  \n",
    "  2. Extensión masiva  \n",
    "  3. Flexión palmar de muñeca (wrist volar flexion)  \n",
    "  4. Dorsiflexión de muñeca (wrist dorsiflexion)  \n",
    "  5. Pronación de antebrazo  \n",
    "  6. Supinación de antebrazo  \n",
    "  7. Reposo (rest) :contentReference[oaicite:5]{index=5}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ⚙️ Instrumentación y protocolo  \n",
    "- Los participantes usaron sensores **EMG** en el antebrazo, sensado **FMG** (forza muscular) y **IMU** (unidad de medición inercial) en la muñeca mientras realizaban los gestos mencionados :contentReference[oaicite:6]{index=6}  \n",
    "- Se diseñó un modelo basado en prototypical networks, junto con selección de características mediante **K-Best** y ajuste de la ventana temporal para mejorar la precisión :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🗂 Estructura de datos / Archivos  \n",
    "- El artículo no detalla almacenamiento público de conjuntos de datos ni la estructura interna (variables, formatos específicos). Sin embargo, se menciona que el código se encuentra disponible en GitHub:\n",
    "  - Repositorio: `https://github.com/HSarwat/Few-Shot-Proto-TL.git` :contentReference[oaicite:8]{index=8}  \n",
    "\n",
    "---\n",
    "\n",
    "##  📊 Características clave  \n",
    "- **Tipo de datos:** Señales biométricas (EMG, FMG, IMU), series temporales  \n",
    "- **Área temática:** Rehabilitación motora post-stroke, inteligencia artificial, control de gestos  \n",
    "- **Tarea asociada:** Reconocimiento de gestos con aprendizaje de transferencia minimalista (one-shot)  \n",
    "- **Modelos comparados:** Prototypical networks vs. redes neuronales tradicionales, LGBM, LDA, SVM; además, enfoques dependientes e independientes del sujeto :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Resultados principales  \n",
    "- **Precisión alcanzada:** 82.2 % en clasificación de gestos con el modelo propuesto  \n",
    "- **Comparativa:**  \n",
    "  - Redes neuronales one-shot TL: 63.17 %  \n",
    "  - Redes neuronales tradicionales: 59.72 %  \n",
    "  - LGBM: 65.09 %  \n",
    "  - LDA: 63.35 %  \n",
    "  - SVM: 54.5 % :contentReference[oaicite:10]{index=10}  \n",
    "- El modelo propuesto se acercó al rendimiento de clasificadores dependientes del sujeto, sólo ligeramente por debajo del SVM (83.84 %) y superior a NN (81.62 %), LGBM (80.79 %) y LDA (74.89 %) :contentReference[oaicite:11]{index=11}  \n",
    "- **K-Best features:** Mejoraron la precisión en 3 de 6 clasificadores  \n",
    "- **Ventana más amplia:** Mejoró el rendimiento en todos los clasificadores con un aumento promedio de precisión del 4.28 % :contentReference[oaicite:12]{index=12}  \n",
    "\n",
    "---\n",
    "\n",
    "##  👩‍🔬 Autores / Referencias  \n",
    "- **Autores principales:** Hussein Sarwat, Amr Alkhashab, Xinyu Song, Shuo Jiang, Jie Jia, Peter B. Shull :contentReference[oaicite:13]{index=13}  \n",
    "- **Registro del estudio:** CHiCTR1800017568 (Chinese Clinical Trial Registry), registro del 4 de agosto de 2018 :contentReference[oaicite:14]{index=14}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🛠 Uso en Python (ejemplo hipotético)\n",
    "El repositorio de GitHub mencionado puede contener datos de ejemplo y scripts. Un posible paso inicial en Python podría ser:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee387aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prototypical_network import ProtoNet  # ejemplo hipotético\n",
    "from data_loader import load_data  # función ficticia\n",
    "\n",
    "# Cargar datos — supongamos que load_data retorna (X_train, y_train), (X_test, y_test)\n",
    "(X_train, y_train), (X_test, y_test) = load_data('ruta_al_dataset')\n",
    "\n",
    "model = ProtoNet(input_dim=X_train.shape[1], num_classes=7)  # según teclas del dataset\n",
    "model.train_one_shot(X_train, y_train)\n",
    "\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"One-shot ProtoNet accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c14b16",
   "metadata": {},
   "source": [
    "# 0️⃣8️⃣ [A Novel Bilateral Data Fusion Approach for EMG-Driven Deep Learning in Post-Stroke Paretic Gesture Recognition](https://pubmed.ncbi.nlm.nih.gov/40573553/)  📚\n",
    "\n",
    "📌 **Publicación:** 11 de junio de 2025 – *Sensors (Basel)* :contentReference[oaicite:0]{index=0}  \n",
    "📌 **Repositorio / Fuente:** Journal *Sensors* (MDPI), acceso abierto (PMC disponible) :contentReference[oaicite:1]{index=1}  \n",
    "📌 **Licencia:** Acceso abierto bajo *Creative Commons* (implícito por política de MDPI) :contentReference[oaicite:2]{index=2}  \n",
    "📌 **DOI:** 10.3390/s25123664 :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "---\n",
    "\n",
    "##  🔎 Descripción general  \n",
    "Este estudio presenta un modelo híbrido de aprendizaje profundo (CNN-LSTM) para reconocer gestos manuales desde señales **EMG superficiales** en pacientes subagudos post-stroke. Se introdujo una metodología novedosa de **fusión bilateral** que incorpora también señales del miembro no afectado para mejorar el entrenamiento. :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "- **Participantes:** 25 pacientes post-stroke (fase subaguda), con dos sesiones de captura espaciadas por al menos una semana :contentReference[oaicite:5]{index=5}  \n",
    "- **Movimientos registrados (7 gestos):**  \n",
    "  1. Reposo (rest)  \n",
    "  2. Puño cerrado (clenched fist)  \n",
    "  3. Pinza índice (index pinch)  \n",
    "  4. Flexión de muñeca (wrist flexion)  \n",
    "  5. Extensión de muñeca (wrist extension)  \n",
    "  6. Mano abierta (spread hand)  \n",
    "  7. Pulgar arriba (thumbs up) :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "---\n",
    "\n",
    "##  ⚙️ Instrumentación y protocolo  \n",
    "- **Electrodos sEMG** bipolares colocados en cuatro regiones del antebrazo:  \n",
    "  - Flexor carpi radialis  \n",
    "  - Flexor carpi ulnaris  \n",
    "  - Región tenar (abductor pollicis brevis & flexor pollicis brevis)  \n",
    "  - Extensor digitorum communis :contentReference[oaicite:7]{index=7}  \n",
    "- **Grabaciones:** Cada paciente realizó al menos 10 intentos por gesto, en ambos miembros (afectado y no afectado). La fusión bilateral se aplicó durante el entrenamiento; las pruebas se realizaron solo con datos del miembro afectado. Se validó mediante validación cruzada de 10 pliegues, repetida 100 veces :contentReference[oaicite:8]{index=8}\n",
    "\n",
    "---\n",
    "\n",
    "##  🗂 Estructura de los datos  \n",
    "El artículo sí describe la metodología y arquitectura, pero **no publica el dataset** ni detalla el formato de los archivos (no disponible públicamente). :contentReference[oaicite:9]{index=9}\n",
    "\n",
    "---\n",
    "\n",
    "##  📊 Resultados principales  \n",
    "- **Precisión del modelo CNN-LSTM híbrido:**  \n",
    "  - *Dataset A:* entre 82.27 % y 85.66 %  \n",
    "  - *Dataset B:* entre 81.69 % y 88.36 % :contentReference[oaicite:10]{index=10}  \n",
    "- **Incrementos con fusión bilateral (especialmente en la subtarea de 3 gestos):**  \n",
    "  - *Dataset A:* de 73.01 % a 78.42 %  \n",
    "  - *Dataset B:* de 77.95 % a 85.69 % :contentReference[oaicite:11]{index=11}  \n",
    "- Se reportan mejoras en precisión, sensibilidad, especificidad y F1-score gracias a la estrategia propuesta :contentReference[oaicite:12]{index=12}\n",
    "\n",
    "---\n",
    "\n",
    "##  👩‍🔬 Autores / Afilaciones  \n",
    "- **Autores principales:** Alexey Anastasiev, Hideki Kadone, Aiki Marushima, Hiroki Watanabe, Alexander Zaboronok, Shinya Watanabe, Akira Matsumura, Kenji Suzuki, Yuji Matsumaru, Hiroyuki Nishiyama, Eiichi Ishikawa :contentReference[oaicite:13]{index=13}  \n",
    "- **Afiliaciones principales:** Universidad de Tsukuba (Japón), incluyendo el Hospital Universitario de Tsukuba, el Centro de Investigación Cybernics, y otros centros asociados :contentReference[oaicite:14]{index=14}\n",
    "\n",
    "---\n",
    "\n",
    "##  🛠 Uso en Python (ejemplo hipotético)  \n",
    "Aunque el dataset no está disponible, este es un ejemplo de cómo podría estructurarse un entrenamiento similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d40ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo teórico: dataset preparado como arrays NumPy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Supón que tienes los siguientes arrays:\n",
    "# X_paretic: datos EMG del miembro afectado (n_samples, timesteps, channels)\n",
    "# X_nonparetic: datos EMG del miembro no afectado\n",
    "# y_labels: etiquetas de los gestos (0–6)\n",
    "\n",
    "# Fusión bilateral: concatenar ambos conjuntos para entrenamiento\n",
    "X_train = np.concatenate([X_paretic, X_nonparetic], axis=0)\n",
    "y_train = np.concatenate([y_labels, y_labels], axis=0)  # mismas etiquetas\n",
    "\n",
    "# Arquitectura CNN-LSTM simplificada\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(None, X_train.shape[2])),\n",
    "    layers.MaxPooling1D(2),\n",
    "    layers.LSTM(128),\n",
    "    layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento (hipotético)\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluación solo con datos del miembro afectado\n",
    "loss, accuracy = model.evaluate(X_paretic, y_labels)\n",
    "print(f\"Precisión en miembro afectado: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd659c",
   "metadata": {},
   "source": [
    "# 0️⃣9️⃣ [Decoding hand and wrist movement intention from chronic stroke survivors with hemiparesis using a user-friendly, wearable EMG-based neural interface](https://pubmed.ncbi.nlm.nih.gov/38218901/)  📚\n",
    "\n",
    "📌 **Publicación:** 13 de enero de 2024 – *Journal of NeuroEngineering and Rehabilitation* (Vol. 21, Artículo 7) :contentReference[oaicite:0]{index=0}  \n",
    "📌 **Repositorio / Fuente:** Journal of NeuroEngineering and Rehabilitation (Open Access, BMC / Springer Nature) :contentReference[oaicite:1]{index=1}  \n",
    "📌 **Licencia:** Acceso abierto bajo *Creative Commons* (implícito por política de BMC) :contentReference[oaicite:2]{index=2}  \n",
    "📌 **DOI:** 10.1186/s12984-023-01301-w :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🔎 Descripción general  \n",
    "El estudio presenta el **sistema NeuroLife® EMG**, una manga portátil con hasta **150 electrodos integrados** para registrar señales de **EMG de alta densidad (HDEMG)**. Se utilizó para decodificar la intención de movimiento del antebrazo, muñeca y mano en **personas con hemiparesia crónica post-stroke**, destacándose también por ser cómodo, portable y de configuración simple para uso doméstico. :contentReference[oaicite:4]{index=4}  \n",
    "\n",
    "- **Participantes:**  \n",
    "  - 7 pacientes crónicos post-stroke (hemiparesia) – edad promedio ~60 años.  \n",
    "  - 7 participantes sanos (edad promedio ~27 años), como grupo control. :contentReference[oaicite:5]{index=5}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ⚙️ Instrumentación y protocolo  \n",
    "- **Dispositivo:** Manga translúcida con hasta 150 electrodos embebidos, fácilmente colocable y conectado a un módulo de adquisición (Intan), ideal para uso en el hogar. :contentReference[oaicite:6]{index=6}  \n",
    "- **Protocolo de grabación:**  \n",
    "  - Presentación visual de imágenes de movimientos en computadora.  \n",
    "  - Cada bloque de grabación duraba ~2-3 min, con períodos de reposo inicial (8 s) y cues (4-6 s) intercalados aleatoriamente. :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  12 Movimientos Decodificados  \n",
    "1. Reposo  \n",
    "2. Mano abierta  \n",
    "3. Cierre de mano (puño)  \n",
    "4. Pinza con índice  \n",
    "5. Flexión de muñeca  \n",
    "6. Extensión de muñeca  \n",
    "7. Pulgar arriba  \n",
    "*(Presumiblemente se incluyen más movimientos hasta completar 12, incluyendo distintos agarres y pronosupinación.)* :contentReference[oaicite:8]{index=8}  \n",
    "\n",
    "---\n",
    "\n",
    "##  📊 Resultados clave  \n",
    "- **Precisión general (12 movimientos + reposo):** 77.1 % ± 5.6 % en participantes con stroke. :contentReference[oaicite:9]{index=9}  \n",
    "- **Pacientes con mano gravemente afectada (sólo 2 movimientos + reposo):** Precisión de 85.4 % ± 6.4 %. :contentReference[oaicite:10]{index=10}  \n",
    "- **Escenarios en línea (online) con dos participantes, comparando tres movimientos + reposo:** Alcanzaron 91.34 % ± 1.53 %. :contentReference[oaicite:11]{index=11}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Usabilidad (evaluación subjetiva)  \n",
    "Los participantes evaluaron varios aspectos del dispositivo en una escala de 1 a 5 (más alto = mejor):  \n",
    "- **Fácil de poner/Quitar (donning/doffing):** 3.60 ± 0.28  \n",
    "- **Comodidad con uso prolongado (>1.5 h):** 4.57 ± 0.20  \n",
    "- **Libertad de movimiento:** 4.07 ± 0.32  \n",
    "- **Confianza de uso en actividades livianas en casa:** 4.07 ± 0.22  \n",
    "- **Satisfacción con el diseño visual:** 4.36 ± 0.24  \n",
    "- **Favorabilidad general:** 4.79 ± 0.15 :contentReference[oaicite:12]{index=12}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Relación entre capacidad motora y decodificación  \n",
    "La precisión del decodificador correlacionó positivamente con la habilidad motora observada. Movimientos con puntuaciones visibles mayores tuvieron mejor precisión, aunque incluso sin movimiento visible (score = 0), el sistema logró distinguir la intención de movimiento. :contentReference[oaicite:13]{index=13}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Datos disponibles / Acceso  \n",
    "Los **datos del estudio no están disponibles públicamente**, pero pueden obtenerse **mediante solicitud razonable a los autores**. :contentReference[oaicite:14]{index=14}  \n",
    "\n",
    "---\n",
    "\n",
    "##  👩‍🔬 Autores / Afiliaciones  \n",
    "- **Autores principales:** Eric C. Meyers, David Gabrieli, Nick Tacca, Lauren Wengerd, Michael Darrow, Bryan R. Schlink, Ian Baumgart, David A. Friedenberg :contentReference[oaicite:15]{index=15}  \n",
    "- **Afiliaciones:** Battelle Memorial Institute (Divisiones de Medical Device Solutions y Health Analytics) :contentReference[oaicite:16]{index=16}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🛠 Uso en Python (hipotético)  \n",
    "Aunque los datos no son accesibles directamente, aquí tienes un ejemplo de cómo podrías estructurar un script si los tuvieras como arrays NumPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Supón que X_stroke y y_labels contienen EMG y etiquetas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stroke, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(f\"Precisión decodificación: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba894d7",
   "metadata": {},
   "source": [
    "# 1️⃣0️⃣ [Improving Fast EMG Classification for Hand Gesture Recognition: A Comprehensive Analysis of Temporal, Spatial, and Algorithm Configurations for Healthy and Post-Stroke Subjects](https://www.preprints.org/manuscript/202505.0374/v1)  📚\n",
    "\n",
    "📌 **Preprint publicación:** Versión 1, enviada el 06 mayo 2025, publicada el 08 mayo 2025 (no revisada por pares) :contentReference[oaicite:0]{index=0}  \n",
    "📌 **Repositorio / Fuente:** Preprints.org (categoría Ingeniería / Bioingeniería) :contentReference[oaicite:1]{index=1}  \n",
    "📌 **Licencia / DOI:** DOI disponible: 10.20944/preprints202505.0374.v1 (licencia no especificada) :contentReference[oaicite:2]{index=2}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🔎 Descripción general  \n",
    "Este estudio evalúa el rendimiento de la clasificación de señales EMG en seis gestos manuales, tanto en individuos sanos como en pacientes post-stroke, explorando cómo el tiempo de adquisición (0,5–4 s) y el número de canales (1 a 4) afectan la precisión, robustez y generalización. Se analizaron múltiples métodos de extracción de características y modelos de aprendizaje automático. :contentReference[oaicite:3]{index=3}  \n",
    "\n",
    "- **Participantes:**  \n",
    "  - Individuos sanos  \n",
    "  - Pacientes post-stroke (para pruebas de generalización cruzada) :contentReference[oaicite:4]{index=4}  \n",
    "- **Gestos clasificados:** seis gestos de mano y dedos, incluidos reposo, flexión/extensión de muñeca, agarre, abducción de dedos y supinación :contentReference[oaicite:5]{index=5}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ⚙️ Instrumentación y protocolo  \n",
    "- **Adquisición en sujetos sanos:**  \n",
    "  - 40 participantes (edad 18–29), balance de género, 3 zurdos, 1 ambidiestro :contentReference[oaicite:6]{index=6}  \n",
    "  - Dispositivo BIOPAC MP36 con 4 canales: músculos extensor carpi ulnaris, flexor carpi ulnaris, extensor carpi radialis, flexor carpi radialis  \n",
    "  - Muestreo EMG a 2 kHz, filtrado banda 5–500 Hz más notch 50 Hz :contentReference[oaicite:7]{index=7}  \n",
    "- **Adquisición en pacientes post-stroke:**  \n",
    "  - Protocolo clínico ético y estandarizado  \n",
    "  - Electrónica similar usando electrodes Ag/AgCl y filtro banda 5–500 Hz, también a 2 kHz, sistema Human SpikerBox :contentReference[oaicite:8]{index=8}  \n",
    "  - Secuencias con preparación (~3 min), gesto (~4 s), reposo (~5 s), 50 repeticiones por gesto (~1 h por paciente) :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Estructura de datos  \n",
    "No se publica directamente el dataset completo; sin embargo, se indica que los datos saludables están disponibles desde la fuente previa. Los datos de pacientes no se facilitan públicamente. Los hiperparámetros optados se incluyen como material suplementario :contentReference[oaicite:10]{index=10}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Métodos – Extracción y clasificación  \n",
    "- **Extracción de características:**  \n",
    "  - Power Spectral Density (PSD) (usando método de Welch, ventana 256 muestras, solapamiento 128, ventana Hanning)  \n",
    "  - Discrete Wavelet Transform (DWT) con wavelet Daubechies (db2), nivel de descomposición 3–4  \n",
    "  - Reducción dimensional por PCA o SVD (98 % varianza retenida) :contentReference[oaicite:11]{index=11}  \n",
    "- **Modelos evaluados:**  \n",
    "  - Random Forest (RF), Support Vector Machine (SVM), Neural Network (MLP)  \n",
    "  - Validación con split de 80 % entrenamiento / 20 % prueba, 5-fold cross-validation, optimización por grid search, seed = 42 :contentReference[oaicite:12]{index=12}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Diseño experimental  \n",
    "- Se probaron 315 configuraciones combinando:  \n",
    "  - 9 ventanas temporales desde 0,5 s hasta 4 s  \n",
    "  - 7 configuraciones de canales (1, 2 o 4 canales combinados)  \n",
    "  - 5 métodos de extracción (PSD, PSD+PCA, PSD+SVD, DWT+PCA, DWT+SVD)  \n",
    "  - 3 algoritmos de clasificación (RF, SVM, NN) :contentReference[oaicite:13]{index=13}  \n",
    "- Además: pruebas de robustez con múltiples particiones, análisis de generalización intra- e inter-paciente :contentReference[oaicite:14]{index=14}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Resultados clave  \n",
    "- **Extracción:** PSD + PCA o PSD + SVD rindió mejor que DWT. RF mostró mayor robustez con DWT. :contentReference[oaicite:15]{index=15}  \n",
    "- **Ventana temporal:** La precisión mejora significativamente al aumentar la ventana de 0,5 s a 2 s, pero se estabiliza más allá de 2 s. :contentReference[oaicite:16]{index=16}  \n",
    "- **Número de canales:** Aumentar de 1 a 2 canales tiene mayor impacto que de 2 a 4. Combinado con ventana ≥ 2 s, mejora sustancial. :contentReference[oaicite:17]{index=17}  \n",
    "- **Robustez:** PSD+PCA presentaba coeficiente de variación bajo (<5 %); DWT+SVD mostró variabilidad mayor (6–10 %). RF fue más consistente mientras que NN fue menos robusto. :contentReference[oaicite:18]{index=18}  \n",
    "- **Generalización en pacientes:**  \n",
    "  - Intra-paciente: hasta ~80 % con 30 muestras por clase; depende del modelo (RF, SVM, NN diferencias sutiles) :contentReference[oaicite:19]{index=19}  \n",
    "  - Inter-paciente: rendimiento se desploma a ~35–40 % → necesidad de adaptación o entrenamiento por usuario :contentReference[oaicite:20]{index=20}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Conclusiones y recomendaciones  \n",
    "- El enfoque ideal para clasificación rápida y precisa de gestos EMG usa:  \n",
    "  - 2 segundos de ventana temporal  \n",
    "  - 2 canales de EMG  \n",
    "  - Extracción PSD + PCA  \n",
    "  - RF como algoritmo robusto :contentReference[oaicite:21]{index=21}  \n",
    "- Para aplicaciones real-time, esta configuración reduce latencia y complejidad sin sacrificar precisión.  \n",
    "- Sistemas personalizados (entrenados por paciente) son más viables que modelos genéricos. Futuras mejoras incluyen deep learning y validación en escenarios reales. :contentReference[oaicite:22]{index=22}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Autores  \n",
    "- Camila Montecinos (primera autora), Jessica Espinoza, Mónica Zamora Zapata, Viviana Meruane, Rubén Fernández :contentReference[oaicite:23]{index=23}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🛠 Uso en Python (hipotético básico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9cf229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# X_emg: datos EMG reshapeados (n_samples, n_features), y_labels: etiquetas\n",
    "pca = PCA(n_components=0.98)\n",
    "X_reduced = pca.fit_transform(X_emg)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "param_grid = {'n_estimators': [100, 150, 200], 'criterion': ['gini','entropy']}\n",
    "grid = GridSearchCV(rf, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejor RF:\", grid.best_params_, \"Accuracy:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea709843",
   "metadata": {},
   "source": [
    "# 1️⃣1️⃣ [U-Limb: A multi-modal, multi-center database on arm motion control in healthy and post-stroke conditions](https://pubmed.ncbi.nlm.nih.gov/34143875/)  📚\n",
    "\n",
    "📌 **Publicación:** 18 de junio de 2021 – *GigaScience* (Vol. 10, Núm. 6) :contentReference[oaicite:0]{index=0}  \n",
    "📌 **Repositorio / Fuente:** GigaScience, Oxford University Press (acceso abierto) :contentReference[oaicite:1]{index=1}  \n",
    "📌 **Licencia / DOI:** DOI: 10.1093/gigascience/giab043 :contentReference[oaicite:2]{index=2}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🔎 Descripción general  \n",
    "U-Limb es una extensa base de datos multimodal y multicéntrica diseñada para profundizar en la comprensión del control motor del brazo:\n",
    "\n",
    "- Participantes: **91 sujetos sanos** y **65 pacientes post-stroke** :contentReference[oaicite:3]{index=3}  \n",
    "- **Tres niveles de datos recopilados**:\n",
    "  1. Actividades cotidianas del miembro superior con registros de **kinemática y señales fisiológicas** (EMG, EEG, ECG)  \n",
    "  2. Comportamiento cinético-táctil durante tareas de manipulación precisas con dispositivo háptico  \n",
    "  3. Actividad cerebral durante el control de la mano, captada por **fMRI** :contentReference[oaicite:4]{index=4}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ⚙️ Instrumentación y protocolo  \n",
    "- **Sensores EMG**: colocados siguiendo directrices SENIAM  \n",
    "- **Kinemática**: marcadores activos para captura de movimiento (motion capture) con precisión anatómica  \n",
    "- **Tareas incluidas**:  \n",
    "  - Virtual Peg Insertion Test — prueba funcional que combina cinemática, fuerza y realidad virtual :contentReference[oaicite:5]{index=5}  \n",
    "  - Estudios en múltiples centros (como Pisa, MHH, TUM, UP), con alineación rigurosa de procedimientos :contentReference[oaicite:6]{index=6}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Estructura del dataset  \n",
    "Aunque el artículo describe ampliamente los datos recogidos, no detalla el formato exacto (archivos, variables o estructura interna). No obstante, está claro que incluye:\n",
    "\n",
    "- Registros EMG, EEG, ECG (señales fisiológicas)  \n",
    "- Datos de movimiento (cinemática)  \n",
    "- Registros de fuerza kinésica durante manipulación  \n",
    "- Datos de actividad cerebral (fMRI) :contentReference[oaicite:7]{index=7}  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Características del dataset  \n",
    "- **Tipo:** Datos multimodales (EMG, EEG, ECG, cinemática, fuerzas, neuroimagen)  \n",
    "- **Área temática:** Neurociencia del movimiento, rehabilitación, robótica asistiva, interacción humano-máquina  \n",
    "- **Tareas potenciales:** Análisis del control motor, rehabilitación personalizada, diseño de prótesis e interfaces adaptativas  \n",
    "\n",
    "---\n",
    "\n",
    "##  ️ Autores y afiliaciones  \n",
    "- **Autores principales:** Giuseppe Averta, Federica Barontini, Vincenzo Catrambone, Sami Haddadin, Giacomo Handjaras, entre otros :contentReference[oaicite:8]{index=8}  \n",
    "- **Centros participantes:**  \n",
    "  - Universidad de Pisa & Istituto Italiano di Tecnologia (Italia)  \n",
    "  - Technical University Munich (Alemania)  \n",
    "  - Hannover Medical School (Alemania)  \n",
    "  - University of Zurich (Suiza), entre otros :contentReference[oaicite:9]{index=9}  \n",
    "\n",
    "---\n",
    "\n",
    "##  🛠 Uso en Python (ejemplo genérico)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo ilustrativo: carga estructurada de datos multimodales\n",
    "import numpy as np\n",
    "\n",
    "# Simulación de carga (ajustar según formato real, por ejemplo .mat, .csv, .npy, etc.)\n",
    "emg = np.load('u_limb_emg_healthy.npy')    # (n_samples, n_channels)\n",
    "eeg = np.load('u_limb_eeg_healthy.npy')    # (n_samples, n_channels)\n",
    "kin = np.load('u_limb_kinematics_healthy.npy')  # (n_samples, ...)\n",
    "\n",
    "print(\"EMG shape:\", emg.shape)\n",
    "print(\"EEG shape:\", eeg.shape)\n",
    "print(\"Kinematics shape:\", kin.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
